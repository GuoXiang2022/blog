---
title: OSTEP
date: 2023-04-23 20:10:08
tags:
    - 操作系统
---

#	1.虚拟化



##	04 抽象:进程

> **关键问题：如何提供有许多CPU可用的假象？**
>
> 虽然只有少量的物理CPU可用，但是操作系统如何提供几乎有无数个CPU可用的假象？

​	操作系统通过虚拟化`virtualizing`CPU来提供这种假象。通过让一个进程只运行一个时间片，然后切换到其他进程，操作系统提供了存在多个虚拟CPU的假象。这就是**时分共享（time sharing）**CPU技术，允许用户如愿运行多个并非进程

​	要实现CPU的虚拟化，且要实现的好，OS需要一些低级机制以及一些高级智能。我们将低级机制称为机制。机制是一些低级方法和协议，实现了所需的功能。例如，稍后学习如何实现**上下文切换（context switch）**,它让OS能够停止运行一个程序，并开始在给定的CPU上运行另一个程序。所有现代OS都采用了这种分时机制。

![image-20230327132632519](/images/mdpic/image-20230327132632519.png)

​	在这些机制以上，OS中有一些智能以**策略（policy）**的形式存在。策略是在OS内做出某种决定的算法。例如；给定一组可能的程序要在CPU上运行，OS应该运行哪个程序？操作系统中的**调度策略（scheduling policy）**会做出这样的决定，可能利用历史信息，工作负载知识以及性能指标来做出决定



####	1.抽象:进程

操作系统为正在进行的程序提供的抽象，就是所谓的**进程（process）**



为了理解构成进程的是什么，我们必须理解它的**机器状态（machine state）**：

- 程序在运行时可以读取或更新的内容
- 在任何时刻，机器的哪些部分对执行该程序很重要？

进行的机器状态有一个明显的组成成分，就是它的内存；指令存在内存中，正在运行的程序读取和写入的数据也在内存中。因此进程可以访问的内存（称为**地址空间**，**address space**）是该进程的一部分

进程的机器状态的另一部分是**寄存器**。许多指令明确地读取和更新寄存器，因此显然，它们对于执行该进程很重要

![image-20230327135118691](/images/mdpic/image-20230327135118691.png)



####	2.进程API

几乎所有现代操作系统都以某种形式提供这些API

- 创建（create）
- 销毁（destroy）
- 等待（wait）
- 其他控制
- 状态（statu）



####	3.进程创建:更多细节

![image-20230317224529699](/images/mdpic/image-20230317224529699.png)



在早期的（或简单的）操作系统中，加载过程**尽早（eagerly）**完成，即在运行程序之前全部完成。现代操作系统**惰性（lazily）**执行该过程，即仅在程序执行期间需要加载的代码或数据片段，才会加载。



要真正理解代码和数据的惰性加载是如何工作的，必须更多地了解**分页**和**交换**的机制，这是我们将来讨论内存虚拟化时要涉及的主题。



将代码和静态数据加载到内存后，操作系统在运行此进程之前还需要执行其他一些操作。必须为程序的**运行时栈（run-time stack 或 stack 又又别称：执行栈，控制栈，机器栈）**分配一些内存。

![cc831af51cec9c7b20c8bd94ac79b4f](C:\Users\guoxiang\AppData\Local\Temp\WeChat Files\cc831af51cec9c7b20c8bd94ac79b4f.png)



C程序使用stack存放局部变量，函数参数和返回地址。OS分配这些内存，并提供给进程。OS也可能会用参数初始化stack。具体来说，它会将参数填入main()函数，即argc和argv数组。



OS也可能为程序的**堆（heap）**分配一些内存；还将执行一些其他初始化任务，特别是**输入/输出（I/O）**相关的任务



通过将代码和静态数据加载到内存中，通过创建和初始化以及执行与I/O设置相关的其他工作，OS现在终于为程序执行搭好了舞台。然后它有最后一项任务：启动程序，在入口处运行，即main()。通过跳转到main()例程，OS将CPU的控制权转移到新创建的进程中，从而程序开始执行。



####	4.进程状态



进程可以处于以下三种状态之一：

- **运行（running）**
- **就绪（ready）**
- **阻塞（blocked）**



![image-20230317225754765](/images/mdpic/image-20230317225754765.png)



####	5.数据结构

OS是一个程序，和其他程序一样，它有一些关键的数据结构来跟踪各种相关的信息。例如，为了跟踪每个进程的状态，OS可能会为所有就绪的进程保留某种**进程列表（process list）**，以及跟踪当前正在运行的进程的一些附加信息。OS还必须以某种方式跟踪被阻塞的进程。当I/O事情完成时，OS应该确保唤醒正确的进程，让它准备好再次运行。



处理运行，就绪，阻塞之外，还有其他一些进程可以处于的状态。有时候系统会有一个**初始（initial）状态**，表示进程在创建时处于的状态。另外，一个程序可以处于已退出但尚未清理的**最终（final）状态**（在基于UNIX的系统中，这称为**僵尸状态**），这个最终状态非常有用，因为它允许其他进程（通常是创建进程的父进程）检查进程的返回代码，并查看刚刚完成的进程是否成功执行（通常，程序成功完成任务时返回零，否则返回非零）



![image-20230327173749301](/images/mdpic/image-20230327173749301.png)











##	05 插叙:进程API

> 本章将介绍系统实践方面的内容，包括OS的API及其使用方式。
>
> **关键问题：如何创建并控制进程**
>
> OS应该提供怎样的接口来创建及控制进程？如何设计这些接口才能既方便又实用？



####	1.fork()系统调用



####	2.wait()系统调用



####	3.exec()系统调用



####	4.为什么这么设计API



####	5.其他API





##	06 机制：受限直接执行

> 为了虚拟化CPU，操作系统需要以某种方式让许多任务共享物理CPU，让它们看起来像是同时运行，基本思想很简单：运行一个进程一段时间，然后运行另一个进程，如此轮换。通过以这种方式**时分共享（time sharing）**CPU，就实现了虚拟化。

然而，在构建这样的虚拟化机制时存在一些挑战

- 性能

  如何在不增加系统开销的情况下实现虚拟化？

- 控制权

  如何有效地运行进程，同时保留对CPU的控制？

在保持控制权的同时获得高性能，这是构建OS的主要挑战之一

![image-20230327194725913](/images/mdpic/image-20230327194725913.png)



####	1.基本技巧：受限直接执行

为了使程序尽可能快地运行，OS开发人员想出了一种技术，我们称之为**受限的直接执行（limited direct execution）**。这个概念的“直接执行”部分很简单：只需直接CPU上运行程序即可：

![image-20230327195231806](/images/mdpic/image-20230327195231806.png)

这种方法在我们虚拟化CPU时产生了一些问题：

- 如果我们只运行一个程序，OS怎么能确保程序不做任何我们不希望它做的事情，同时高效地运行它？
- 当我们运行一个程序时，OS如何让它停下来并切换到另一个进程，从而实现虚拟化CPU所需的时空共享？



####	2.问题1：受限制的操作

采用受保护的控制权转移，来实现进程的受限操作。

![image-20230327200059414](/images/mdpic/image-20230327200059414.png)

- 用户模式（user mode）：在该模式下运行的代码会受到限制。例如；进程不能发出I/O请求。这样做会导致处理器引发异常，OS可能会终止进程
- 内核模式（kernel mode）：OS就以这种模式运行。在该模式下，运行的代码可以做它喜欢的事情，包括特权操作；如发出I/O请求和执行所有类型的受限指令

如果用户希望执行某种特权操作（如从磁盘读取），应该怎么做？

为了实现这一点，几乎所有的现代硬件都提供了用户程序执行**系统调用**的能力

要执行系统调用，程序必须执行特殊的**陷阱（trap）指令**。该指令同时跳入内核并将特权级别提升到内核模式。一旦进入内核，系统就可以执行任何需要的特权操作，从而为调用进程执行所需的工作。完成后，OS调用一个特殊的**陷阱返回（return -from-trap）指令**，该指令返回到发起调用的用户程序中，同时将特权级别降低，回到用户模式

![image-20230327201321859](/images/mdpic/image-20230327201321859.png)

还有一个重要的细节没讨论：陷阱如何知道在 OS 内运行哪些代码？

显然，发起调用的 过程不能指定要跳转到的地址（就像你在进行过程调用时一样），这样做让程序可以跳转到 内核中的任意位置，这显然是一个糟糕的主意（想象一下跳到访问文件的代码，但在权限 检查之后。实际上，这种能力很可能让一个狡猾的程序员令内核运行任意代码序列）。 因此内核必须谨慎地控制在陷阱上执行的代码。 

内核通过在启动时设置**陷阱表（trap table）**来实现。当机器启动时，它在特权（内核）模式下执行，因此可以根据需要自由配置机器硬件。操作系统做的第一件事，就是告诉硬件在 发生某些异常事件时要运行哪些代码。例如，当发生硬盘中断，发生键盘中断或程序进行系 统调用时，应该运行哪些代码？操作系统通常通过某种特殊的指令，通知硬件这些陷阱处理 程序的位置。一旦硬件被通知，它就会记住这些处理程序的位置，直到下一次重新启动机器， 并且硬件知道在发生系统调用和其他异常事件时要做什么（即跳转到哪段代码）。 最后再插一句：能够执行指令来告诉硬件陷阱表的位置是一个非常强大的功能。因此， 你可能已经猜到，这也是一项特权（privileged）操作。如果你试图在用户模式下执行这个 指令，硬件不会允许，你可能会猜到会发生什么（提示：再见，违规程序）。思考问题：如 果可以设置自己的陷阱表，你可以对系统做些什么？你能接管机器吗？

![image-20230327201642422](/images/mdpic/image-20230327201642422.png)



####	3.问题2：在进程之间切换

如果一个进程在CPU上运行，就意味着OS没有在CPU上运行。

OS如何重新获得CPU的**控制权（regain control）**,以便它可以在进程之间切换？



- **协作方式：等待系统调用**

  过去的某些系统采用的一种方式，在这种风格下，OS相信系统的进程会合理运行。运行时间长的进程被假定会定期放弃CPU，以便OS可以决定其他任务。

  大多数进程通过系统调用，将CPU的控制权转移给OS。例如打开文件并随后读取文件，或者创建进程。这样的系统通常包括一个显示的yield系统调用，它什么也不做，只是将控制权交给OS，以便系统可以运行其他进程。

  如果某个进程进入无限循环，并且从不进行系统调用，会发生什么情况？

- **非协作方式：操作系统进行控制**

  如果进程拒绝进行系统调用（也不出错），从而将控制权交还给OS，那么OS无法做任何事情。在协作模式下，当进程陷入无限循环时，唯一的办法就是重启电脑。

  如何在没有协作的情况下获得控制权？

  答案很简单：**时钟中断（timer interrupt）**。时钟设备可以编程为每隔几毫秒产生一次中断。产生中断时，当前正在运行的进程停止，OS中预先配置的中断处理程序（interrupt handler）会运行。此时，OS重新获得CPU的控制权，因此可以做它想做的事：停止当前进程，并启动另一个进程。

  ![image-20230327203535076](/images/mdpic/image-20230327203535076.png)



**保存和恢复上下文**

既然OS已经重新获得了控制权，无论通过系统调用协作，还是通过时钟中断强制执行，都必须决定：是继续允许当前正在运行的进程，还是切换到另一个进程。这个决定是由**调度程序（scheduler）**做出的，它是OS的一部分



如果决定进行切换，OS就会执行一些底层代码，即所谓的**上下文切换（context switch）**。上下文切换在概念上很简单：操作系统要做的就是为当前正在执行的进程保存一些寄存器 的值（例如，到它的内核栈），并为即将执行的进程恢复一些寄存器的值（从它的内核栈）。 这样一来，操作系统就可以确保最后执行从陷阱返回指令时，不是返回到之前运行的进程， 而是继续执行另一个进程。







####	4.担心并发吗

在系统调用期间发生时钟中断会发生什么？ 处理一个中断时发生另一个中断会发生什么？

这正是第二部分关于并发的主题。

![image-20230327205307488](/images/mdpic/image-20230327205307488.png)



####	5.小结：

我们已经表述了一些实现CPU虚拟化的关键底层机制，并将其统称为受限直接执行（limited direct execution）。基本思路很简单；就让你想运行的程序在CPU上运行，但首先确保设置好硬件，以便在没有OS帮助的情况下限制进程可以执行的操作（用户模式和内核模式）。

通过类似的方式，OS在启动时设置陷阱处理程序（陷阱表）并启动时钟中断，然后仅在受限模式下运行进程。这样做，OS能确信进程可以高效运行，只在执行特权操作，或者当它们独占CPU时间过长并因此需要切换时，才需要OS干预。



##	07 进程调度：介绍

> 现在，运行进程的底层机制（如上下文切换）应该清楚了。现在，我们还不清楚OS调度程序采用的上层**策略（policy）**。接下来会介绍一系列的调度策略

关键问题：如何开发调度策略

什么是关键假设？哪些指标非常重要？



####	1.工作负载假设

探讨可能的策略范围之前，我们先做一些简化假设。这些假设与系统中运行的进程有关，有时候统称为**工作负载（workload）**。确定工作负载是构建调度策略的关键部分。工作负载了解得越多，你的策略就越优化。

我们这里做的工作负载的假设是不切实际的，但这没问题（目前），因为我们将来会放宽这些假定，并最终开发出我们所谓的 一个完全可操作的调度准则（a fully-operational scheduling discipline）。 

我们对操作系统中运行的进程（有时也叫工作任务）做出如下的假设：

1. 每一个工作运行相同的时间。
2. 所有的工作同时到达。
3. 一旦开始，每个工作保持运行直到完成。
4. 所有的工作只是用 CPU（即它们不执行 IO 操作）。
5. 每个工作的运行时间是已知的。

显然，这些假设很多是不现实的。





####	2.调度指标

**周转时间（turnaround time）：**

任务的周转时间定义为，任务完成时间减去任务到达系统的时间。更正式的周转时间定义是：` 周转时间` = `完成时间`− `到达时间`

因为我们假设所有的任务在同一时间到达，那么 到达时间= 0，因此 周转时间= 完成时间。随 着我们放宽上述假设，这个情况将改变。 你应该注意到，周转时间是一个**性能（performance）指标**，这将是本章的首要关注点。





####	3.先进先出（FIFO）

我们可以实现的最基本的算法，被称为**先进先出（First In First Out 或 FIFO）调度**，有 时候也称为**先到先服务（First Come First Served 或 FCFS）**。

FIFO 有一些积极的特性：它很简单，而且易于实现。而且，对于我们的假设，它的效果很好。

但是它的周转时间是一个致命的缺点：

![image-20230329231826725](/images/mdpic/image-20230329231826725.png)

这个问题通常被称为**护航效应（convoy effect）**，一些耗时较少的潜在资源消费者被排在重量级的资源消费者之后。这个调度方案可能让你想起在杂货店只有一个排队队伍的时候，如果看到前面的人装满 3 辆购物车食品并且掏出了支票本，你感觉如何？



####	4.最短任务优先（SJF）

事实证明，一个非常简单的方法解决了这个问题。实际上这是从运筹学中借鉴的一个想法，然后应用到计算机系统的任务调度中。这个新的调度准则被称为**最短任务优先（Shortest Job First，SJF）**，该名称应该很容易记住，因为它完全描述了这个策略： **先运行最短的任务，然后是次短的任务，如此下去。**

如果我们放宽假设；工作可以随时到达，而不是同时到达，这又会出现新的问题：

![image-20230329232058743](/images/mdpic/image-20230329232058743.png)

从图中可以看出，即使 B 和 C 在 A 之后不久到达，它们仍然被迫等到 A 完成，从而遭遇同样的**护航问题**。



####	5.最短完成时间优先（STCF）

为了解决这个问题，我们继续放宽假设（工作必须保持运行直到完成）。

鉴于我们先前关于**时钟中断**和**上下文切换**的讨论，当 B 和 C 到达时，调度程序当然可以做其他事情：它可以**抢占（preempt）**工作 A，并 决定运行另一个工作，或许稍后继续工作 A。根据我们的定义，**SJF 是一种非抢占式 （non-preemptive）调度程序，因此存在上述问题。**



幸运的是，有一个调度程序完全就是这样做的：向 SJF 添加抢占，称为**最短完成时间优先（Shortest Time-to-Completion First，STCF）**或抢占式最短作业优先（Preemptive Shortest Job First ，PSJF）调度程序。每当新工作进入系统时，它就会确定剩余工作和新工作中， 谁的剩余时间最少，然后调度该工作。因此，在我们的例子中，STCF 将抢占 A 并运行 B 和 C 以完 成。只有在它们完成后，才能调度 A 的剩余时间：

![image-20230329232329320](/images/mdpic/image-20230329232329320.png)

如果所用工作同时到达，SJF是最优的。如果不是同时到达，显然STCF是最优的





####	6.新度量指标：响应时间

用户会坐在终端前面，同时也要求系统的交互性好。因此，一个新的度量标准诞生了：**响应时间（response time）**。 响应时间定义为从任务到达系统到首次运行的时间。更正式的定义是： T 响应时间= T 首次运行−T 到达时间



STCF 和相关方法在响应时间上并不是很好。例如，如果 3 个工作同时到 达，第三个工作必须等待前两个工作全部运行后才能运行。这种方法虽然有很好的周转时 间，但对于响应时间和交互性是相当糟糕的。假设你在终端前输入，不得不等待 10s 才能看 到系统的回应，只是因为其他一些工作已经在你之前被调度：你肯定不太开心。



####	7.轮转

为了解决这个问题，我们将介绍一种新的调度算法，通常被称为**轮转（Round-Robin， RR）调度**。基本思想很简单：RR 在一个时间片（time slice，有时称为调度量子，scheduling quantum）内运行一个工作，然后切换到运行队列中的下一个任务，而不是运行一个任务直到结束。它反复执行，直到所有任务完成。因此，RR 有时被称为时间切片（time-slicing）。 请注意，时间片长度必须是时钟中断周期的倍数。因此，如果时钟中断是每 10ms 中断一次， 则时间片可以是 10ms、20ms 或 10ms 的任何其他倍数。

![image-20230329232629440](/images/mdpic/image-20230329232629440.png)

如你所见，时间片长度对于 RR 是至关重要的。越短，RR 在响应时间上表现越好。然而，时间片太短是有问题的：**突然上下文切换的成本将影响整体性能**。因此，系统设计者 需要权衡时间片的长度，使其足够长，以便**摊销（amortize）**上下文切换成本，而又不会使 系统不及时响应。

![image-20230329232722545](/images/mdpic/image-20230329232722545.png)

如果响应时间是我们的唯一指标，那么带有合理时间片的 RR，就会是非常好的调度程序。但是我们老朋友的周转时间呢？



如果周转时间是我们的指标，那么 RR 确实是最糟糕的策略之一。直观地说，这应该是有意义的：RR 所做的正是延伸每个工作，只运行每个工作一小段时间，就转 向下一个工作。因为周转时间只关心作业何时完成，RR 几乎是最差的，在很多情况下甚至比简单的 FIFO 更差。



更一般地说，任何公平（fair）的政策（如 RR），即在小规模的时间内将 CPU 均匀分配到活动进程之间，在周转时间这类指标上表现不佳。事实上，这是固有的权衡：**如果你愿意不公平，你可以运行较短的工作直到完成，但是要以响应时间为代价。如果你重视公平 性，则响应时间会较短，但会以周转时间为代价。**这种权衡在系统中很常见。鱼和熊掌不能兼得。

接下来我们还有两个假设需要放宽：作业没有I/O和每个作业运行时间是已知的。



####	8.结合I/O

调度程序显然要在工作发起 I/O 请求时做出决定，因为当前正在运行的作业在 I/O 期间不会使用 CPU，它被阻塞等待 I/O 完成。如果将 I/O 发送到硬盘驱动器，则进程可能会被阻塞几毫秒或更长时间，具体取决于驱动器当前的 I/O 负载。因此，这时调度程序应该在 CPU 上安排另一项工作。 调度程序还必须在 I/O 完成时做出决定。发生这种情况时，会产生中断，操作系统运行 并将发出 I/O 的进程从阻塞状态移回就绪状态。当然，它甚至可以决定在那个时候运行该项 工作。操作系统应该如何处理每项工作？



一种常见的方法是将 A 的每个 10ms 的子工作视为一项独立的工作。因此，当系统启动 时，它的选择是调度 10ms 的 A，还是 50ms 的 B。对于 STCF，选择是明确的：选择较短的 一个，在这种情况下是 A。然后，A 的工作已完成，只剩下 B，并开始运行。然后提交 A 的一个新子工作，它抢占 B 并运行 10ms。这样做可以实现**重叠（overlap）**，一个进程在等 待另一个进程的 I/O 完成时使用 CPU，系统因此得到更好的利用：

![image-20230329233148222](/images/mdpic/image-20230329233148222.png)



这样我们就看到了调度程序可能如何结合 I/O。通过将每个 CPU 突发作为一项工作， 调度程序确保“交互”的进程经常运行。当这些交互式作业正在执行 I/O 时，其他 CPU 密 集型作业将运行，从而更好地利用处理器。



####	9.无法预知

有了应对 I/O 的基本方法，我们来到最后的假设：调度程序知道每个工作的长度。如前所述，这可能是可以做出的最糟糕的假设。事实上，在一个通用的操作系统中（比如我们 所关心的操作系统），操作系统通常对每个作业的长度知之甚少。



####	10.小结

我们介绍了调度的基本思想，并开发了两类方法。第一类是**运行最短的工作**，从而**优化周转时间**。第二类是**交替运行所有工作**，从而**优化响应时间**。但很难做到“鱼与熊掌兼 得”，这是系统中常见的、固有的折中。我们也看到了如何将 I/O 结合到场景中，但仍未解决操作系统根本无法看到未来的问题。稍后，我们将看到如何通过构建一个调度程序，利用最近的历史预测未来，从而解决这个问题。这个调度程序称为**多级反馈队列**，是第 8 章的主题。





##	08 调度：多级反馈队列

> 多级反馈队列（Multi-level Feedback Queue，MLFQ），是一种著名的调度方法，由1962年首次提出。
>
> 多级反馈队列需要解决两方面的问题；首先，它要**优化周转时间**，这通过先执行短工作来完成。然而，OS通常不知道工作要运行多久，这又是**SJF**或**STCF**等算法所必需的。其次，MLFQ希望给交互用户很好的交互体验，因此需要**降低响应时间**。然而，像**轮转**这样的算法虽然降低了响应时间，周转时间却很差。所以这里的问题是：**通常我们对进程一无所知，应该如何构建调度程序来实现这些目标？调度程序如何在运行过程中学习进程的特征，从而做出更好的调度决策？**

关键问题：没有完备的知识如何调度？

没有工作长度的先验知识，如何设计一个能同时减少响应时间和周转时间的调度程序？

![image-20230330123004855](/images/mdpic/image-20230330123004855.png)



####	1.MLFQ：基本规则

MLFQ 中有许多独立的**队列（queue）**，每个队列有不同的**优先级（priority level）**。任何 时刻，一个工作只能存在于一个队列中。MLFQ 总是优先执行较高优先级的工作（即在较高级队列中的工作）。

当然，每个队列中可能会有多个工作，因此具有同样的优先级。在这种情况下，我们 就对这些工作采用**轮转调度**。

因此，MLFQ 调度策略的关键在于如何**设置优先级**。MLFQ 没有为每个工作指定不变的优先情绪而已，而是根据观察到的行为调整它的优先级。例如，如果一个工作不断放弃 CPU 去等待键盘输入，这是交互型进程的可能行为，MLFQ 因此会让它保持高优先级。相 反，如果一个工作长时间地占用 CPU，MLFQ 会降低其优先级。通过这种方式，MLFQ 在 进程运行过程中学习其行为，从而利用工作的历史来预测它未来的行为。

至此，我们得到了 MLFQ 的两条基本规则。 

**规则 1：如果 A 的优先级 > B 的优先级，运行 A（不运行 B）。** 

**规则 2：如果 A 的优先级 = B 的优先级，轮转运行 A 和 B。**

![image-20230330123310581](/images/mdpic/image-20230330123310581.png)



####	2.尝试1：如何改变优先级

我们必须决定，在一个工作的生命周期中，MLFQ 如何改变其优先级（在哪个队列中）。 要做到这一点，我们必须记得工作负载：既有运行时间很短、频繁放弃 CPU 的交互型工作， 也有需要很多 CPU 时间、响应时间却不重要的长时间计算密集型工作。下面是我们第一次 尝试优先级调整算法：

**规则 3：工作进入系统时，放在最高优先级（最上层队列）。** 

**规则 4a：工作用完整个时间片后，降低其优先级（移入下一个队列）。** 

**规则 4b：如果工作在其时间片以内主动释放 CPU， 则优先级不变。**



**实例 1：单个长工作**

我们来看一些例子。首先，如果系统中有一个需要长 时间运行的工作，看看会发生什么：

![image-20230330123904908](/images/mdpic/image-20230330123904908.png)

从这个例子可以看出，该工作首先进入最高优先级 （Q2）。执行一个 10ms 的时间片后，调度程序将工作的优先 级减 1，因此进入 Q1。在 Q1 执行一个时间片后，最终降低优先级进入系统的最低优先级 （Q0），一直留在那里。相当简单，不是吗？



**实例 2：来了一个短工作** 

再看一个较复杂的例子，看看 MLFQ 如何近似 SJF。在这个例子中，有两个工作：A 是一个长时间运行的 CPU 密集型工作，B 是一个运行时间很短的交互型工作。假设 A 执行 一段时间后 B 到达。会发生什么呢？对 B 来说，MLFQ 会近似于 SJF 吗？

![image-20230330123943611](/images/mdpic/image-20230330123943611.png)

通过这个例子，你大概可以体会到这个算法的一个主要目标：**如果不知道工作是短工作还是长工作，那么就在开始的时候假设其是短工作，并赋予最高优先级。**如果确实是短 工作，则很快会执行完毕，否则将被慢慢移入低优先级队列，而这时该工作也被认为是长 工作了。通过这种方式，MLFQ 近似于 SJF。



**实例 3：如果有 I/O 呢** 

看一个有 I/O 的例子。根据上述规则 4b，如果进程在时间片用完之前主动放弃 CPU， 则保持它的优先级不变。这条规则的意图很简单：假设交互型工作中有大量的 I/O 操作（比 如等待用户的键盘或鼠标输入），它会在时间片用完之前放弃 CPU。在这种情况下，我们不想处罚它，只是保持它的优先级不变：

![image-20230330124100713](/images/mdpic/image-20230330124100713.png)





**当前 MLFQ 的一些问题**

至此，我们有了基本的 MLFQ。它看起来似乎相当不错，长工作之间可以公平地分享CPU，又能给短工作或交互型工作很好的响应时间。然而，这种算法有一些非常严重的缺点；

首先，会有**饥饿（starvation）问题**。如果系统有“太多”交互型工作，就会不断占用 CPU，导致**长工作永远无法得到 CPU（它们饿死了）**。即使在这种情况下，我们希望这些长工作也能有所进展。 

其次，聪明的用户会重写程序，**愚弄调度程序（game the scheduler）**。愚弄调度程序指 的是用一些卑鄙的手段欺骗调度程序，让它给你远超公平的资源。上述算法对如下的攻击束手无策：**进程在时间片用完之前，调用一个 I/O 操作（比如访问一个无关的文件），从而主动释放CPU。如此便可以保持在高优先级，占用更多的 CPU 时间。**做得好时（比如，每 运行 99%的时间片时间就主动放弃一次 CPU），工作可以几乎独占 CPU。

最后，**一个程序可能在不同时间表现不同**。**一个计算密集的进程可能在某段时间表现为一个交互型的进程。用我们目前的方法，它不会享受系统中其他交互型工作的待遇。**





####	3.尝试2：提升优先级

让我们试着改变之前的规则，看能否避免**饥饿问题**。要让 CPU 密集型工作也能取得一些进展（即使不多），我们能做些什么？



一个简单的思路是**周期性地提升（boost）所有工作的优先级**。可以有很多方法做到， 但我们就用最简单的：将所有工作扔到最高优先级队列。于是有了如下的新规则。



 **规则 5：经过一段时间 S，就将系统中所有工作重新加入最高优先级队列。**



新规则一下**解决了两个问题**。首先，进程不会饿死——在最高优先级队列中，它会以**轮转**的方式，与其他高优先级工作分享 CPU，从而最终获得执行。其次，如果一个 CPU 密集型工作变成了交互型，当它优先级提升时，调度程序会正确对待它。



![image-20230330124506545](/images/mdpic/image-20230330124506545.png)

当然，添加时间段 S 导致了明显的问题：S 的值应该如何设置？德高望重的系统研究员John Ousterhout曾将这种值称为“**巫毒常量（voo-doo constant）**”，因为似乎需要一些黑魔法才能正确设置。**如果 S 设置得太高，长工作会饥饿；如果设置得太低，交互型工作又得不到合适的 CPU 时间比例。**



####	4.尝试3：更好的计时方式

现在还有一个问题要解决：**如何阻止调度程序被愚弄？**可以看出，这里的元凶是规则 4a 和 4b，导致工作在时间片以内释放 CPU，就保留它的优先级。那么应该怎么做？ 

这里的解决方案，是为 MLFQ 的每层队列提供更完善的 CPU **计时方式（accounting**）。 调度程序应该记录一个进程在某一层中消耗的总时间，而不是在调度时重新计时。只要进程用完了自己的配额，就将它降到低一优先级的队列中去。不论它是一次用完的，还是拆成很多次用完。因此，我们重写规则 4a 和 4b。

**规则 4：一旦工作用完了其在某一层中的时间配额（无论中间主动放弃了多少次 CPU），就降低其优先级（移入低一级队列）。**



来看一个例子。图 8.6 对比了在规则 4a、4b 的策略下（左图），以及在新的规则 4（右图）的策略下，同样试图愚弄调度程序的进程的表现。没有规则 4 的保护时，进程可以在 每个时间片结束前发起一次 I/O 操作，从而垄断 CPU 时间。有了这样的保护后，**不论进程 的 I/O 行为如何，都会慢慢地降低优先级，因而无法获得超过公平的 CPU 时间比例。**

![image-20230330130301278](/images/mdpic/image-20230330130301278.png)



####	5.MLFQ调优及其他问题

关于 MLFQ 调度算法还有一些问题；其中一个大问题是如何配置一个调度程序，例如， **配置多少队列？每一层队列的时间片配置多大？为了避免饥饿问题以及进程行为改变，应该多久提升一次进程的优先级？**这些问题都没有显而易见的答案，因此只有利用对工作负载的经验，以及后续对调度程序的调优，才会导致令人满意的平衡。







####	6.MLFQ：小结

本章介绍了一种调度方式，名为**多级反馈队列（MLFQ）**。你应该已经知道它为什么叫 这个名字——它有多级队列，并**利用反馈信息决定某个工作的优先级。以史为鉴：关注进程的一贯表现，然后区别对待。**



本章包含了一组优化的 MLFQ 规则。为了方便查阅，我们重新列在这里。

- **规则 1：如果 A 的优先级 > B 的优先级，运行 A（不运行 B）。** 
- **规则 2：如果 A 的优先级 = B 的优先级，轮转运行 A 和 B。** 
- **规则 3：工作进入系统时，放在最高优先级（最上层队列）。** 
- **规则 4：一旦工作用完了其在某一层中的时间配额（无论中间主动放弃了多少次 CPU），就降低其优先级（移入低一级队列）。** 
- **规则 5：经过一段时间 S，就将系统中所有工作重新加入最高优先级队列。**



MLFQ 有趣的原因是：**它不需要对工作的运行方式有先验知识，而是通过观察工作的运行来给出对应的优先级。**通过这种方式，MLFQ 可以同时满足各种工作的需求：**对于短时间运行的交互型工作，获得类似于 SJF/STCF 的很好的全局性能，同时对长时间运行的 CPU 密集型负载也可以公平地、不断地稳步向前。**因此，许多系统使用某种类型的 MLFQ 作为自己的基础调度程序，包括类 BSD UNIX 系统、Solaris以及 Windows NT 和其后的 Window 系列操作系统。







##	09 调度：比例分配

在本章中，我们来看一个不同类型的调度程序——**比例份额（proportional-share）调度**程序，有时也称为**公平份额（fair-share）调度**程序。比例份额算法基于一个简单的想法：**调度程序的最终目标，是确保每个工作获得一定比例的 CPU 时间，而不是优化周转时间和响应时间。**







####	7.小结

本章介绍了比例份额调度的概念，并简单讨论了两种实现：**彩票调度**和**步长调度**。 **彩票调度通过随机值，聪明地做到了按比例分配。步长调度算法能够确定的获得需要的比例**。虽然两者都很有趣，但由于一些原因，并没有作为 CPU 调度程序被广泛使用。**一 个原因是这两种方式都不能很好地适合 I/O；另一个原因是其中最难的票数分配问题并没有确定的解决方式，**例如，如何知道浏览器进程应该拥有多少票数？通用调度程序（像前面讨论的 MLFQ 及其他类似的 Linux 调度程序）做得更好，因此得到了广泛的应用。 

结果，比例份额调度程序只有在这些问题可以相对容易解决的领域更有用（例如容易 确定份额比例）。例如在虚拟（virtualized）数据中心中，你可能会希望分配 1/4 的 CPU 周 期给 Windows 虚拟机，剩余的给 Linux 系统，比例分配的方式可以更简单高效。



##	10 多处理器调度（高级）







##	11 关于CPU虚拟化的总结对话

我学到了什么？

首先，我了解了OS如何虚拟化CPU。为了理解这一点，我必须了解一些重要的机制：陷阱和陷阱处理程序，时钟中断以及OS和硬件在进程间切换时如何谨慎地保存和恢复状态。OS希望确保控制机器；虽然它希望程序能够尽可能地高效运行`受限直接执行（limited direct execution）`，但OS也希望能够对错误或恶意地程序说不。这也许就是我们将OS视为资源管理器地原因

其次，在机制之上地策略，可以建立一个既像SJF又像RR地调度程序（MLFQ）。





##	12 关于内存虚拟化的对话



##	13 抽象：地址空间



/images/mdpic/

####	1.早期系统

从内存来看，早期的机器并没有提供多少抽象给用户。基本上，机器的物理内存看起来如下所示：

![image-20230424172914543](/images/mdpic/image-20230424172914543.png)



####	2.多道程序和时分共享

过了一段时间，由于机器昂贵，人们开始更有效地共享机器。因此，多道程序 （multiprogramming）系统时代开启，其中多个进程在给定时间准备运行，比如当有 一个进程在等待 I/O 操作的时候，操作系统会切换这些进程，这样增加了 CPU 的有效利用 率（utilization）。那时候，效率（efficiency）的提高尤其重要，因为每台机器的成本是数十 万美元甚至数百万美元，但很快，人们开始对机器要求更多，***分时系统***的时代诞生了。 具体来说，许多人意识到批量计算的局限性，尤其是程序员本身，他们厌倦了长时间的（因此也是低效率的）编程—调试循环。***交互性（interactivity）***变得很重要，因为许多用户可能同时在使用机器，每个人都在等待（或希望）他们执行的任务及时响应。 一种实现时分共享的方法，是让一个进程单独占用全部内存运行一小段时间，然后停止它，并将它所有的状态信息保存在磁盘上（包含所有的物理内存），加载其他进程的状态信息，再运行一段时间，这就实现了某种比较粗糙的机器共享。 遗憾的是，这种方法有一个问题：太慢了，特别是当内存增长的时候。虽然保存和恢复寄存器级的状态信息（程序计数器、通用寄存器等）相对较快，但将全部的内存信息保存到磁盘就太慢了。

因此，在进程切换的时候，我们仍然将进程信息放在内存中，这样操作系统 可以更有效率地实现时分共享。 在图 13.2 中，有 3 个进程（A、B、C），每个进程拥有从 512KB 物理内存中切出来给它们的一小部分内存。假定只有一 个 CPU，操作系统选择运行其中一个进程（比如 A），同时其 他进程（B 和 C）则在队列中等待运行。 随着时分共享变得更流行，人们对操作系统又有了新的要求。特别是多个程序同时驻留在内存中，使***保护（protection）*** 成为重要问题。人们不希望一个进程可以读取其他进程的内存，更别说修改了。

![image-20230424172931651](/images/mdpic/image-20230424172931651.png)

####	3.地址空间

然而，我们必须将这些烦人的用户的需求放在心上。因此操作系统需要提供一个易用 （easy to use）的物理内存抽象。这个抽象叫作***地址空间（address space）***，是运行的程序看到的系统中的内存。理解这个基本的操作系统内存抽象，是了解内存虚拟化的关键。

一个进程的地址空间包含运行的程序的所有内存状态。比如：程序的代码（code，指令） 必须在内存中，因此它们在地址空间里。当程序在运行的时候，利用栈（stack）来保存当前的函数调用信息，分配空间给局部变量，传递参数和函数返回值。最后，堆（heap）用于 管理动态分配的、用户管理的内存，就像你从 C 语言中调用 malloc()或面向对象语言（如 C ++ 或 Java）中调用 new 获得内存。当然，还有其他的东西（例如，静态初始化的变量），但现在假设只有这 3 个部分：**代码、栈和堆**。

![image-20230424172948199](/images/mdpic/image-20230424172948199.png)

在上图的例子中，我们有一个很小的地址 空间（只有 16KB）。程序代码位于地址空间的顶部（在本例中从 0 开始，并且装入到地址空间 的前 1KB）。代码是静态的（因此很容易放在内存中），所以可以将它放在地址空间的顶部，我们知道程序运行时不再需要新的空间。 接下来，在程序运行时，地址空间有两个区域可能增长（或者收缩）。它们就是堆（在顶部） 和栈（在底部）。把它们放在那里，是因为它们都希望能够增长。通过将它们放在地址空间的两端，我们可以允许这样的增长：它们只需要在相反的方向增长。因此堆在代码（1KB） 之下开始并向下增长（当用户通过 malloc()请求更多内存时），栈从 16KB 开始并向上增长（当用户进行程序调用时）。然而，堆栈和堆的这种放置方法只是一种约定，如果你愿意， 可以用不同的方式安排地址空间 [稍后我们会看到，当多个线程（threads）在地址空间中共 存时，就没有像这样分配空间的好办法了]。 当然，当我们描述地址空间时，所描述的是操作系统提供给运行程序的抽象（abstract）。 程序不在物理地址 0～16KB 的内存中，而是加载在任意的物理地址。回顾图 13.2 中的进程 A、B 和 C，你可以看到每个进程如何加载到内存中的不同地址。因此问题来了：

![image-20230424173005007](/images/mdpic/image-20230424173005007.png)

当操作系统这样做时，我们说操作系统在***虚拟化内存（virtualizing memory）***，因为运行的程序认为它被加载到特定地址（例如 0）的内存中，并且具有非常大的地址空间（例如 32 位或 64 位）。现实很不一样。 例如，当图 13.2 中的进程 A 尝试在地址 0（我们将称其为虚拟地址，virtual address） 执行加载操作时，然而操作系统在硬件的支持下，出于某种原因，必须确保不是加载到物理地址 0，而是物理地址 320KB（这是 A 载入内存的地址）。这是内存虚拟化的关键，这是世界上每一个现代计算机系统的基础。

![image-20230424173017223](/images/mdpic/image-20230424173017223.png)

####	4.目标

在这一章中，我们触及操作系统的工作——虚拟化内存。操作系统不仅虚拟化内存， 还有一定的风格。为了确保操作系统这样做，我们需要一些目标来指导。以前我们已经看 过这些目标（想想本章的前言），我们会再次看到它们，但它们肯定是值得重复的。 

虚拟内存（VM）系统的一个主要目标是***透明（transparency）***。操作系统实现虚拟内存的方式，应该让运行的程序看不见。因此，程序不应该感知到内存被虚拟化的事实，相反，程序的行为就好像它拥有自己的私有物理内存。在幕后，操作系统（和硬件）完成了 所有的工作，让不同的工作复用内存，从而实现这个假象。

虚拟内存的另一个目标是***效率（efficiency）***。操作系统应该追求虚拟化尽可能高效 （efficient），包括时间上（即不会使程序运行得更慢）和空间上（即不需要太多额外的内存 来支持虚拟化）。在实现高效率虚拟化时，操作系统将不得不依靠硬件支持，包括 TLB 这样 的硬件功能（我们将在适当的时候学习）。 

最后，虚拟内存第三个目标是***保护（protection）***。操作系统应确保进程受到保护（protect）， 不会受其他进程影响，操作系统本身也不会受进程影响。当一个进程执行加载、存储或指令提取时，它不应该以任何方式访问或影响任何其他进程或操作系统本身的内存内容（即 在它的地址空间之外的任何内容）。因此，保护让我们能够在进程之间提供隔离（isolation） 的特性，每个进程都应该在自己的独立环境中运行，避免其他出错或恶意进程的影响

![image-20230424173034427](/images/mdpic/image-20230424173034427.png)

在接下来的章节中，我们将重点介绍虚拟化内存所需的基本机制（mechanism），包括硬件和操作系统的支持。我们还将研究一些较相关的策略（policy），你会在操作系统中遇到它们，包括如何管理可用空间，以及在空间不足时哪些页面该释放。通过这些内容，你会逐渐理解现代虚拟内存系统真正的工作原理。



####	5.小结

我们介绍了操作系统的一个重要子系统：虚拟内存。虚拟内存系统负责为程序提供一个巨大的，稀疏的，私有的地址空间的假象，其中保存了程序的所有指令和数据。操作系统在专门硬件的帮助下，通过每一个虚拟内存的索引，将其转换为物理地址，物理内存根据获得的物理地址去获取所需要的信息。操作系统会同时对许多进程执行此操作，并且确保程序之间相互不受影响，也不会影响操作系统。整个方法需要大量的机制（很多底层机制）和一些关键策略。我们将自底向上，先描述关键机制。





##	14 插叙：内存操作API



在UNIX/C程序中，如何分配和管理内存？哪些错误需要避免？



####	1.内存类型

在运行一个 C 程序的时候，会分配两种类型的内存。

第一种称为栈内存，它的申请和释放操作是编译器来隐式管理的，所以有时也称为**自动（automatic）内存**。

第二种称为堆内存，其中所有的申请和释放操作都由程序员显式地完成。

```c
int *x = (int*)malloc(sizeof(int));
```

关于这一小段代码有两点说明。首先，你可能会注意到栈和堆的分配都发生在这一行： 首先编译器看到指针的声明（int * x）时，知道为一个整型指针分配空间，随后，当程序调 malloc 时，它会在堆上请求整数的空间，函数返回这样一个整数的地址（成功时，失败时则返回 NULL），然后将其存储在栈中以供程序使用。



####	2.malloc()调用

malloc 函数非常简单：传入要申请的堆空间的大小，它成功就返回一个指向新申请空间的指针，失败就返回 NULL。

man 手册展示了使用 malloc 需要怎么做，在命令行输入 man malloc，你会看到：

```c
#include <stdlib.h>
...
void *malloc(size_t size);
```

从这段信息可以看到，只需要包含头文件 stdlib.h 就可以使用 malloc 了。但实实上， 甚至都不需这样做，因为 C 库是 C 程序默但链接的，其中就有 mallock()的代码，加上这个 头文件只是让编译器检查你是否正确调用了 malloc()（即传入参数的数目正确且类型正确）。 malloc 只需要一个 size_t 类型参数，该参数表示你需要多少个字我。然而，大多数程 序员并不会直接传入数字（比如 10）。实实上，这样做会被但为是不太好的形式。替代方案 是使用各种函数和宏。例如，为了给双精度浮点数分配空间，只要这样：

```c
double *d = (double*)malloc(sizeof(double));
```

啊，好多double！对malloc()的调用使用sizeof() 操作符去申请正确大小的空间。在C 中，这通常被但为是编译时操作符，意味着这个大小是在**编译时**就已知道，因此被替换成一个数（在本例中是 8，对于 double），作为 malloc()的参数。出于这个原因，sizeof() 被正确地但为是一个操作符，而不是一个函数调用（函数调用在运行时发生）。

另一个需要注意的地方是使用字符串。如果为一个字符串声明空间，请使用以下习惯 用法：malloc(strlen(s) + 1)，它使用函数 strlen()获取字符串的长度，并加上 1，以便为字符 串结束符留出空间。这里使用 sizeof()可能会导致麻烦。

你也许还注意到 malloc()返回一个指向 void 类型的指针。这样做只是 C 中传回地址的 方式，让程序员决定如何处理它。程序员将进一步使用所谓的强制类型转换（cast），在我们上面的示例中，程序员将返回类型的 malloc()强制转换为指向 double 的指针。强制类型转换实实上没干什么事，只是告诉编译器和其他可能正在读你的代码的程序员：“是的，我知道我在做什么。”通过强制转换 malloc()的结果，程序员只是在给人一些信心，强制转换不是程序正确所必须的。

####	3.free()调用

事实证明，分配内存是等式的简单部分。知道何时、如何以及是否释放内存是困难的部分。要释放不再使用的堆内存，程序员只需调用 free()：

```c
int *x = malloc(10 * sizeof(int));
...
free(x); 
```

该函数接受一个参数，即一个由 malloc()返回的指针。 因此，你可能会注意到，分配区域的大小不会被用户传入，必须由内存分配库本身记 录追踪。



####	4.常见错误

在使用 malloc()和 free()时会出现一些常见的错误。以下是我们在教授本科操作系统课程时反复看到的情形。所有这些例子都可以通过编译器的编译并运行。对于构建一个正确 的 C 程序来说，通过编译是必要的，但这远远不够，你会懂的（通常在吃了很多苦头之后）。 实实上，正确的内存管理就是这样一个问题，许多新语言都支持自动内存管理（automatic memory management）。在这样的语言中，当你调用类似 malloc()的机制来分配内存时（通常 用 new 或类似的东西来分配一个新对象），你永远不需要调用某些东西来释放空间。实实上， 垃圾收集器（garbage collector）会运行，找出你不再引用的内存，替你释放它。

- 忘记分配内存

  很多函数在调用前，都希望你为它们分配内存，例如strcpy（dst，src）：

  ```c
  char *src = "hello";
  char *dst;				// unallocated!
  strcpy(dst,src);		// segmentation fault!
  ```

- 没有分配足够的内存

  有时称为**缓冲区溢出（buffer overflow）**：

  ```c
  char *src = "hello";
  char *dst = (char*)malloc(strlen(src));		// too small!
  strcpy(dst,src);
  ```

  奇怪的是，这个程序通常看起来会正确运行，这取决于如何实现 malloc 和许多其他细节。在某些情况下，当字符串拷贝执行时，它会在超过分配空间的末尾处写入一个字我， 但在某些情况下，这是无害的，可能会覆盖不再使用的变量。在某些情况下，这些溢出可 能具有令人难以置信的危害，实实上是系统中许多安全漏洞的来源。在其他情况下， malloc 库总是分配一些额外的空间，因此你的程序实实上不会在其他某个变量的值上涂写， 并且工作得很好。还有一些情况下，该程序确实会发生故障和崩溃。因此，我们学到了另 一个宝贵的教训：即使它正确运行过一次，也不意味着它是正确的。

- 忘记初始化分配的内存

  如果有指针指向未初始化的内存，称该指针为野指针。

  在这个错误中，你正确地调用 malloc()，但忘记在新分配的数据类型中填写一些值。不要这样做！如果你忘记了，你的程序最终会遇到未初始化的读取（uninitialized read），它从 堆中读取了一些未知值的数据。谁知道那里可能会有什么？如果走运，读到的值使程序仍然有效（例如，零）。如果不走运，会读到一些随机和有害的东西。

- 忘记释放内存

  也称为**内存泄露（memory leak）**，如果忘记释放内存，就会发生。在长时间运行的应用程序或系统（如操作系统本身）中，这是一个巨大的问题，因为缓慢泄露的 内存会导致内存不足，此时需要重新启动。因此，一般来说，当你用完一段内存时，应该 确保释放它。请注意，使用垃圾收集语言在这里没有什么帮助：如果你仍然拥有对某块内 存的引用，那么垃圾收集器就不会释放它，因此即使在较现代的语言中，内存泄露仍然是 一个问题。 在某些情况下，不调用 free()似乎是合理的。例如，你的程序运行时间很短，很但就会 退出。在这种情况下，当进程死亡时，操作系统将清理其分配的所有页面，因此不会发生 内存泄露。虽然这肯定“有效”（请参阅后面的补充），但这可能是一个坏习惯，所以请谨 慎选择这样的策略。长远来看，作为程序员的目标之一是养成良好的习惯。其中一个习惯 是理解如何管理内存，并在 C 这样的语言中，释放分配的内存块。即使你不这样做也可以 逃脱惩罚，建议还是养成习惯，释放显式分配的每个字我。

- 在用完之前释放内存

  有时候程序会在用完之前释放内存，这种错误称为空悬指针（dangling pointer），正如你猜测的那样，这也是一件坏事。随后的使用可能会导致程序崩溃或覆盖有效的内存（例如，你调用了 free()，但随后再次调用 malloc()来分配其他内容，这重新利用了错误释放的内存），即***未定义行为（undefine behavior）***。

- 重复释放内存

  程序有时还会不止一次地释放内存，这被称为***重复释放（double free）***。这样做的结果是***定义行为（undefine behavior）***。正如你所能想象的那样，内存分配库可能会感到困惑，并且会做各种奇怪的 事情，崩溃是常见的结果。

- 错误地调用free

  我们讨论的最后一个问题是 free()的调用错误。毕竟，free()期望你只传入之前从 malloc() 得到的一个指针。如果传入一些其他的值，坏事就可能发生（并且会发生）。因此，这种无 效的释放（invalid free）是危险的，当然也应该避免。

![image-20230424173247104](/images/mdpic/image-20230424173247104.png)



**小结**

如你所见，有很多方法滥用内存。由于内存出错很常见，整个工具生态圈已经开发出来，可以帮助你在代码中找到这些问题。请查看 purify 和 valgrind，在帮助你找到与内存有关的问题的根源方面，两者都非常出色。一旦你习惯于使用这些强大的工具， 就会想知道，没有它们时，你是如何活下来的。



####	5.底层操作系统支持

你可能已经注意到，在讨论 malloc()和 free()时，我们没有讨论系统调用。原因很简单： 它们不是系统调用，而是库调用。因此，malloc 库管理虚拟地址空间内的空间，但是它本身 是建立在一些系统调用之上的，这些系统调用会进入操作系统，来请求本多内存或者将一 些内容释放回系统。 

一个这样的系统调用叫作 brk，它被用来改变程序分断（break）的位置：堆结束的位置。 它需要一个参数（新分断的地址），从而根据新分断是大于还是小于当前分断，来增加或减 小堆的大小。另一个调用 sbrk 要求传入一个增量，但目的是类似的。 请注意，你不应该直接调用 brk 或 sbrk。它们被内存分配库使用。如果你尝试使用它们， 很可能会犯一些错误。建议坚持使用 malloc()和 free()。 最后，你还可以通过 mmap()调用从操作系统获取内存。通过传入正确的参数，mmap() 可以在程序中创建一个匿名（anonymous）内存区域——这个区域不与任何特定文件相关联， 而是与交换空间（swap space）相关联，稍后我们将在虚拟内存中详细讨论。这种内存也可 以像堆一样对待并管理。阅读 mmap()的手册页以获取本多详细信息。



####	6.其他调用

内存分配库还支持一些其他调用。例如，calloc()分配内存，并在返回之前将其置零。 如果你但为内存已归零并忘记自己初始化它，这可以防止出现一些错误（请参阅 14.4 我中 “忘记初始化分配的内存”的内容）。当你为某些东西（比如一个数组）分配空间，然后需要添加一些东西时，例程 realloc()也会很有用：realloc()创建一个新的本大的内存区域，将 旧区域复制到其中，并返回新区域的指针。



####	7.小结

我们介绍了一些处理内存分配的 API。与往常一样，我们只介绍了基本知识。本多细节可在其他地方获得。



##	15 机制：地址转换

在实现CPU虚拟化时，我们遵循的一般准则被称为受限直接访问（Limited Direct Execution LDE）。

LDE背后的想法很简单：让程序运行的大部分指令直接访问硬件，只在一些关键点（如进程发起系统调用或发生时钟中断）由操作系统介入来确保“在正确的时间，正确的地点，做正确的事”。为了实现高效的虚拟化，操作系统应该尽量让程序自己运行，同时通过在关键点的及时介入，来保持对硬件的控制。高效和控制是现代操作系统的两个主要目标。

在实现虚拟内存时，我们将追求类似的战略，在实现高效和控制的同时，提供期望的虚拟化。高效决定了我们要利用硬件的支持，这在开始的时候非常初级（如使用一些寄存器），但会变得相当复杂（比如我们会讲到的 TLB、页表等）。控制意味着操作系统要确保应用程序只能访问它自己的内存空间。因此，要保护应用程序不会相互影响，也不会影响操作系统，我们需要硬件的帮助。最后，我们对虚拟内存还有一点要求，即灵活性。具体来说，我们希望程序能以任何方式访问它自己的地址空间，从而让系统更容易编程。所以， 关键问题在于：

![image-20230424191725088](/images/mdpic/image-20230424191725088.png)

我们利用了一种通用技术，有时被称为基于硬件的地址转换（hardware-based address translation），简称为地址转换。它可以看成是受限直接执行这种一般方法的补充。利用地址转换，硬件对每次内存访问进行处理（即指令获取、数据读取或写入），**将指令中的虚拟（virtual）地址转换为数据实际存储的物理（physical）地址**。因此， 在每次内存引用时，硬件都会进行地址转换，将应用程序的内存引用重定位到内存中实际的位置。

当然，仅仅依靠硬件不足以实现虚拟内存，因为它只是提供了底层机制来提高效率。 操作系统必须在关键的位置介入，设置好硬件，以便完成正确的地址转换。因此它必须**管理内存（manage memory）**，记录被占用和空闲的内存位置，并明智而谨慎地介入，保持对 内存使用的控制。

同样，所有这些工作都是为了创造一种美丽的假象：每个程序都拥有私有的内存，那 里存放着它自己的代码和数据。虚拟现实的背后是丑陋的物理事实：许多程序其实是在同 一时间共享着内存，就像 CPU（或多个 CPU）在不同的程序间切换运行。通过虚拟化， 操作系统（在硬件的帮助下）将丑陋的机器现实转化成一种有用的、强大的、易于使用的 抽象。

####	1.假设

我们对内存虚拟化的第一次尝试非常简单，甚至有点可笑。如果你觉得可笑就笑吧， 很快就轮到操作系统嘲笑你了。当你试图理解 TLB 的换入换出、多级页表，和其他技术一样有奇迹之处的时候。不喜欢操作系统嘲笑你？很不幸，但这就是操作系统的运行方式。 具体来说，我们先假设用户的地址空间必须连续地放在物理内存中。同时，为了简单， 我们假设地址空间不是很大，具体来说，小于物理内存的大小。最后，假设每个地址空间 的大小完全一样。别担心这些假设听起来不切实际，我们会逐步地放宽这些假设，从而得到现实的内存虚拟化。



####	2.一个例子

实现地址转换需要什么？为什么需要？我们可以先看一个简单的例子：

```c
void func(){
    int x;
    x = x + 3;
}
```

汇编代码可能如下：

```c
movl 0x0(%ebx), %eax ;load 0+ebx into eax
addl $0x03, %eax ;add 3 to eax register
movl %eax, 0x0(%ebx) ;store eax back to mem
```

<img src="/images/mdpic/image-20230424193021238.png" alt="image-20230424193021238" style="zoom:65%;"/>



如果这 3 条指令执行，从进程的角度来看，发生了以下几次内存访问： 

- 从地址 128 获取指令； 
- 执行指令（从地址 15KB 加载数据）； 
- 从地址 132 获取命令； 
- 执行命令（没有内存访问）； 
- 从地址 135 获取指令； 
- 执行指令（新值存入地址 15KB）。

从程序的角度来看，它的地址空间（address space）从 0 开始到 16KB 结束。它包含的所有内存引用都应该在这个范围内。然而，对虚拟内存来说，操作系统希望将这个进程地址空间放在物理内存的其他位置，并不一定从地址 0 开始。因此我们遇到了如下问题：

怎样在内存中重定位这个进程，同时对该进程透明（transparent）？

怎么样提供一种虚拟地址空间从 0 开始的假象，而实际上地址空间位于另外某个物理地址？

![image-20230424193309946](/images/mdpic/image-20230424193309946.png)

####	3.动态（基于硬件）重定位

为了更好地理解基于硬件的地址转换，我们先来讨论它的第一次应用。在 20 世纪 50 年代后期，它在首次出现的时分机器中引入，那时只是一个简单的思想，称为***基址加界限机制（base and bound）***，有时又称为***动态重定位（dynamic relocation）***，我们将互换使用这两个术语。

具体来说，每个 CPU 需要两个硬件寄存器：基址（base）寄存器和界限（bound）寄存器，有时称为限制（limit）寄存器。这组基址和界限寄存器，让我们能够将地址空间放在物理内存的任何位置，同时又能确保进程只能访问自己的地址空间。 采用这种方式，在编写和编译程序时假设地址空间从零开始。但是，当程序真正执行时， 操作系统会决定其在物理内存中的实际加载地址，并将起始地址记录在基址寄存器中。在上面的例子中，操作系统决定加载在物理地址 32KB 的进程，因此将基址寄存器设置为这个值。 当进程运行时，有趣的事情发生了。现在，该进程产生的所有内存引用，都会被处理器通过以下方式转换为物理地址：

```bash
physical address = virtual address + base
```

进程中使用的内存引用都是***虚拟地址（virtual address）***，硬件接下来将虚拟地址加上基址寄存器中的内容，得到***物理地址（physical address）***，再发给内存系统。 为了更好地理解，让我们追踪一条指令执行的情况。具体来看前面序列中的一条指令：

```c
movl 0x0(%ebx), %eax 
```

程序计数器（PC）首先被设置为 128。当硬件需要获取这条指令时，它先将这个值加上基址寄存器中的 32KB(32768)，得到实际的物理地址 32896，然后硬件从这个物理地址获取指令。 接下来，处理器开始执行该指令。这时，进程发起从虚拟地址 15KB 的加载，处理器同样将虚拟地址加上基址寄存器内容（32KB），得到最终的物理地址 47KB，从而获得需要的数据。

将虚拟地址转换为物理地址，这正是所谓的**地址转换（address translation）技术**。也就是说，**硬件取得进程认为它要访问的地址，将它转换成数据实际位于的物理地址**。**由于这种重定位是在运行时发生的**，而且我们甚至可以在进程开始运行后改变其地址空间，这种技术一般被称为**动态重定位（dynamic relocation）**。

![image-20230424194129723](/images/mdpic/image-20230424194129723.png)

现在你可能会问，界限（限制）寄存器去哪了？不是基址加界限机制吗？正如你猜测的那样，界限寄存器提供了访问保护。在上面的例子中，界限寄存器被置为 16KB。如果进程需要访问超过这个界限或者为负数的虚拟地址，CPU 将触发异常，进程最终可能被终止。

界限寄存器的用处在于，它确保了进程产生的所有地址都在进程的地址“界限”中。 **这种基址寄存器配合界限寄存器的硬件结构是芯片中的（每个 CPU 一对）**。有时我们将 CPU 的这个负责地址转换的部分统称为***内存管理单元（Memory Management Unit，MMU）***。 随着我们开发更复杂的内存管理技术，MMU 也将有更复杂的电路和功能。 

关于界限寄存器再补充一点，它通常有两种使用方式。在一种方式中（像上面那样），它记录地址空间的大小，硬件在将虚拟地址与基址寄存器内容求和前，就检查这个界限。另一种方式是界限寄存器中记录地址空间结束的物理地址，硬件在转化虚拟地址到物理地址之后才去检查这个界限。这两种方式在逻辑上是等价的。简单起见，我们这里假设采用第一种方式。

**转换示例** 

为了更好地理解基址加界限的地址转换的详细过程，我们来看一个例子。设想一个进程拥有 4KB 大小地址空间（是的，小得不切实际），它被加载到从 16KB 开始的物理内存中。 一些地址转换结果见表 15.1。

![image-20230424194500331](/images/mdpic/image-20230424194500331.png)

4400 < 4kb == 4096

从例子中可以看到，通过基址加虚拟地址（可以看作是地址空间的偏移量）的方式， 很容易得到物理地址。虚拟地址“过大”或者为负数时，会导致异常。

![image-20230424194521346](/images/mdpic/image-20230424194521346.png)



####	4.硬件支持：总结

我们来总结一下需要的硬件支持（见表 15.2）。首先，正如在 CPU 虚拟化的章节中提到的，我们需要两种 CPU 模式。操作系统在**特权模式（privileged mode，或内核模式，kernel mode）**，可以访问整个机器资源。应用程序在**用户模式（user mode）**运行，只能做有限的操作。只要一个位，也许保存在处理器状态字（processor status word）中，就能说明当前的 CPU 运行模式。在一些特殊的时刻（如系统调用、异常或中断），CPU 会切换状态。

![image-20230424194928012](/images/mdpic/image-20230424194928012.png)

硬件还必须提供***基址和界限寄存器（base and bounds register）***，因此每个 CPU 的内存管理单元（Memory Management Unit，MMU）都需要这两个额外的寄存器。用户程序运行时， 硬件会转换每个地址，即将用户程序产生的虚拟地址加上基址寄存器的内容。硬件也必须能检查地址是否有用，通过界限寄存器和 CPU 内的一些电路来实现。 

硬件应该提供一些特殊的指令，用于修改基址寄存器和界限寄存器，允许操作系统在切换进程时改变它们。这些指令是特权（privileged）指令，只有在内核模式下，才能修改这些寄存器。想象一下，如果用户进程在运行时可以随意更改基址寄存器，那么用户进程 可能会造成严重破坏。想象一下吧！然后迅速将这些阴暗的想法从你的头脑中赶走，因为 它们很可怕，会导致噩梦。 

最后，在用户程序尝试非法访问内存（越界访问）时，CPU必须能够产生异常（exception）。 在这种情况下，CPU 应该阻止用户程序的执行，并安排操作系统的“越界”异常处理程序 （exception handler）去处理。操作系统的处理程序会做出正确的响应，比如在这种情况下终 止进程。类似地，如果用户程序尝试修改基址或者界限寄存器时，CPU 也应该产生异常， 并调用“用户模式尝试执行特权指令”的异常处理程序。CPU 还必须提供一种方法，来通知它这些处理程序的位置，因此又需要另一些特权指令。

总结：

- CPU需要提供两种模式：内核模式和用户模式
- 硬件提供基址和界限寄存器
- 硬件提供一些特殊指令用于修改基址和界限寄存器
- CPU必须能够产生异常，应对用户程序尝试非法访问内存





####	5.操作系统的问题

为了支持动态重定位，硬件添加了新的功能，使得操作系统有了一些必须处理的新问题。硬件支持和操作系统管理结合在一起，实现了一个简单的虚拟内存。具体来说，在一 些关键的时刻操作系统需要介入，以实现基址和界限方式的虚拟内存，见表 15.3。

![image-20230424195541882](/images/mdpic/image-20230424195541882.png)



第一，在进程创建时，操作系统必须采取行动，为进程的地址空间找到内存空间。由于我们假设每个进程的地址空间小于物理内存的大小，并且大小相同，这对操作系统来说很容易。它可以把整个物理内存看作一组槽块，标记了空闲或已用。当新进程创建时，操作系统检索这个数据结构（常被称为***空闲列表，free list***），为新地址空间找到位置，并将其标记为已用。如果地址空间可变，那么生活就会更复杂，我们将在后续章节中讨论。 我们来看一个例子。在图 15.2 中，操作系统将物理内存的第一个槽块分配给自己，然后将例子中的进程重定位到物理内存地址 32KB。另两个槽块（16～32KB，48～64KB）空闲，因此空闲列表（free list）就包含这两个槽块。 

第二，在进程终止时（正常退出，或因行为不端被强制终止），操作系统也必须做一些 工作，回收它的所有内存，给其他进程或者操作系统使用。在进程终止时，操作系统会将这些内存放回到空闲列表，并根据需要清除相关的数据结构。 

第三，在上下文切换时，操作系统也必须执行一些额外的操作。每个 CPU 毕竟只有一 个基址寄存器和一个界限寄存器，但对于每个运行的程序，它们的值都不同，因为每个程 序被加载到内存中不同的物理地址。因此，在切换进程时，操作系统必须保存和恢复基础和界限寄存器。具体来说，当操作系统决定中止当前的运行进程时，它必须将当前基址和界限寄存器中的内容保存在内存中，放在某种每个进程都有的结构中，如***进程结构（process structure）***或***进程控制块（Process Control Block，PCB）***中。类似地，当操作系统恢复执行某个进程时（或第一次执行），也必须给基址和界限寄存器设置正确的值。

需要注意，当进程停止时（即没有运行），操作系统可以改变其地址空间的物理位置，这很容易。要移动进程的地址空间，操作系统首先让进程停止运行，然后将地址空间拷贝到新位置，最后更新保存的基址寄存器（在进程结构中），指向新位置。当该进程恢复执行时，它的（新）基址寄存器会被恢复，它再次开始运行，显然它的指令和数据都在新的内存位置了。 

第四，操作系统必须提供异常处理程序（exception handler），或要一些调用的函数，像 上面提到的那样。操作系统在启动时加载这些处理程序（通过特权命令）。例如，当一个进 程试图越界访问内存时，CPU 会触发异常。在这种异常产生时，操作系统必须准备采取行动。通常操作系统会做出充满敌意的反应：终止错误进程。操作系统应该尽力保护它运行的机器，因此它不会对那些企图访问非法地址或执行非法指令的进程客气。再见了，行为 不端的进程，很高兴认识你。



####	6.小结

本章通过虚拟内存使用的一种特殊机制，即地址转换（address translation），扩展了受限直接访问的概念。利用地址转换，操作系统可以控制进程中的所有内存访问，确保访问在地址空间的界限内。

这个技术高效的关键是硬件支持，**硬件快速地将所有内存访问操作中的虚拟地址（进程自己看到的内存位置）转换为物理地址（实际位置）**。所有的这一切对进程来说都是**透明的**，进程并不知道自己使用的内存引用已经被重定位，制造了美妙的假象。

我们还看到了一种特殊的虚拟化方式，**称为基址加界限的动态重定位**。基址加界限的虚拟化方式非常高效，因为只需要很少的硬件逻辑，就可以将虚拟地址和基址寄存器加起来，并检查进程产生的地址没有越界。基址加界限也提供了保护，操作系统和硬件的协作， 确保没有进程能够访问其地址空间之外的内容。保护肯定是操作系统最重要的目标之一。 没有保护，操作系统不可能控制机器（如果进程可以随意修改内存，它们就可以轻松地做出可怕的事情，比如重写陷阱表并完全接管系统）。

遗憾的是，这个简单的动态重定位技术有效率低下的问题。例如，从图 15.2 中可以看到，重定位的进程使用了从 32KB 到 48KB 的物理内存，但由于该进程的栈区和堆区并不很大， 导致这块内存区域中大量的空间被浪费。这种浪费通常称为***内部碎片（internal fragmentation）***， 指的是已经分配的内存单元内部有未使用的空间（即碎片），造成了浪费。在我们当前的方式中，即使有足够的物理内存容纳更多进程，但我们目前要求将地址空间放在固定大小的槽块中，因此会出现内部碎片。所以，我们需要更复杂的机制，以便更好地利用物理内存，避免内部碎片。第一次尝试是将基址加界限的概念稍稍泛化，得到***分段（segmentation）***的概念， 我们接下来将讨论。





##	16 分段

到目前为止，我们一直假设将所有进程的地址空间完整地加载到内存中。利用基址和界限寄存器，操作系统很容易将不同进程重定位到不同的物理内存区域。但是，对于这些内存区域，你可能已经注意到一件有趣的事：栈和堆之间，有一大块“空闲”空间。从图 16.1 中可知，如果我们将整个地址空间放入物理内存，那么栈和堆之间的空间并没有被进程使用，却依然占用了实际的物理内存。因此，简单的通过基址寄存器和界限寄存器实现的虚拟内存很浪费。另外，如果剩余物理内存无法提供连续区域来放置完整的地址空间，进程便无法运行。这种基址加界限的方式看来并不像我们期望的那样灵活。因此：

![image-20230424200758277](/images/mdpic/image-20230424200758277.png)

####	1.分段：泛化的基址/界限

为了解决这个问题，分段（segmentation）的概念应运而生。分段并不是一个新概念， 它甚至可以追溯到 20 世纪 60 年代初期。这个想法很简单，**在 MMU 中引入不止一个基址和界限寄存器对，而是给地址空间内的每个逻辑段（segment）一对**。一个段只是地址空间里的一个连续定长的区域，在典型的地址空间里有 3 个逻辑不同的段：代码、栈 和堆。分段的机制使得操作系统能够将不同的段放到不同的物理内存区域，从而避免了虚拟地址空间中的未使用部分占用物理内存。

我们来看一个例子。假设我们希望将图 16.1 中的地址空间放入物理内存。通过给每个段一对基址和界限寄存器，可以将每个段独立地放入物理内存。如图 16.2 所示，64KB 的物理内存中放置了 3 个段（为操作系统保留 16KB）。 从图中可以看到，只有已用的内存才在物理内存中分配空间，因此可以容纳巨大的地址空间，其中包含大量未使用的地址空间（有时又称为***稀疏地址空间，sparse address spaces***）。 你会想到，需要 MMU 中的硬件结构来支持分断：在这种情况下，需要一组 3 对基址和界限寄存器。表 16.1 展示了上面的例子中的寄存器值，每个界限寄存器记录了一个段的大小。

<img src="/images/mdpic/image-20230424201505727.png" alt="image-20230424201505727" style="zoom: 67%;" />

如表 16.1 所示，代码段放在物理地址 32KB，大小是 2KB。堆在 34KB，大小也是 2KB。 利用图 16.1 中的地址空间，我们来看一个地址转换的例子。假设现在要引用虚拟地址 100（在代码段中），MMU 将基址值加上偏移量（100）得到实际的物理地址：100 + 32KB = 32868。然后它会检查该地址是否在界限内（100 小于 2KB），发现是的，于是发起对物理地址 32868 的引用。

![image-20230424201556151](/images/mdpic/image-20230424201556151.png)

来看一个堆中的地址，虚拟地址 4200（同样参考图 16.1）。如果用虚拟地址 4200 加上堆的基址（34KB），得到物理地址 39016，这不是正确的地址。我们首先应该先减去堆的偏 移量，即该地址指的是这个段中的哪个字节。因为堆从虚拟地址 4K（4096）开始，4200 的 偏移量实际上是 4200 减去 4096，即 104，然后用这个偏移量（104）加上基址寄存器中的 物理地址（34KB），得到真正的物理地址 34920。

如果我们试图访问非法的地址，例如 7KB，它超出了堆的边界呢？你可以想象发生的情况： **硬件会发现该地址越界，因此陷入操作系统，很可能导致终止出错进程。**这就是每个 C 程序员 都感到恐慌的术语的来源：**段异常（segmentation violation）或段错误（segmentation fault）**。



####	2.我们引用哪个段

硬件在地址转换时使用段寄存器。它如何知道段内的偏移量，以及地址引用了哪个段？ 一种常见的方式，有时称为***显式（explicit）***方式，就是用虚拟地址的开头几位来标识不同的段，VAX/VMS 系统使用了这种技术。在我们之前的例子中，有 3 个段，因此需要两位来标识。如果我们用 14 位虚拟地址的前两位来标识，那么虚拟地址如下所示：

![image-20230424201825480](/images/mdpic/image-20230424201825480.png)

那么在我们的例子中，如果前两位是 00，硬件就知道这是属于代码段的地址，因此使 用代码段的基址和界限来重定位到正确的物理地址。如果前两位是 01，则是堆地址，对应地，使用堆的基址和界限。下面来看一个 4200 之上的堆虚拟地址，进行进制转换，确保弄清楚这些内容。虚拟地址 4200 的二进制形式如下：

![image-20230424201847082](/images/mdpic/image-20230424201847082.png)

从图中可以看到，前两位（01）告诉硬件我们引用哪个段。剩下的 12 位是段内偏移： 0000 0110 1000（即十六进制 0x068 或十进制 104）。因此，硬件就用前两位来决定使用哪个段寄存器，然后用后 12 位作为段内偏移。偏移量与基址寄存器相加，硬件就得到了最终的物理地址。请注意，偏移量也简化了对段边界的判断。我们只要检查偏移量是否小于界限， 大于界限的为非法地址。因此，如果基址和界限放在数组中（每个段一项），为了获得需要的物理地址，硬件会做下面这样的事：

```c
// get top 2 bits of 14-bit VA
Segment = (VirtualAddress & SEG_MASK) >> SEG_SHIFT
// now get offset
Offset = VirtualAddress & OFFSET_MASK
if (Offset >= Bounds[Segment])
RaiseException(PROTECTION_FAULT)
else
PhysAddr = Base[Segment] + Offset
Register = AccessMemory(PhysAddr) 
```

在我们的例子中，可以为上面的常量填上值。具体来说，SEG_MASK 为 0x3000， SEG_SHIFT 为 12，OFFSET_MASK 为 0xFFF。 你或许已经注意到，上面使用两位来区分段，但实际只有 3 个段（代码、堆、栈），因 此有一个段的地址空间被浪费。因此有些系统中会将堆和栈当作同一个段，因此只需要一 位来做标识。 硬件还有其他方法来决定特定地址在哪个段。在***隐式（implicit）***方式中，硬件通过地址产生的方式来确定段。例如，如果地址由程序计数器产生（即它是指令获取），那么地址在代码段。如果基于栈或基址指针，它一定在栈段。其他地址则在堆段。

####	3.栈怎么办

到目前为止，我们一直没有讲地址空间中的一个重要部分：栈。在表 16.1 中，栈被重 定位到物理地址 28KB。但有一点关键区别，它反向增长。在物理内存中，它始于 28KB， 增长回到 26KB，相应虚拟地址从 16KB 到 14KB。地址转换必须有所不同。 首先，我们需要一点硬件支持。除了基址和界限外，硬件还需要知道段的增长方向（用 一位区分，比如 1 代表自小而大增长，0 反之）。在表 16.2 中，我们更新了硬件记录的视图。

![image-20230424203322545](/images/mdpic/image-20230424203322545.png)

硬件理解段可以反向增长后，这种虚拟地址的地址转换必须有点不同。下面来看一个栈虚拟地址的例子，将它进行转换，以理解这个过程： 在这个例子中，假设要访问虚拟地址 15KB，它应该映射到物理地址 27KB。该虚拟地址的二进制形式是：11 1100 0000 0000（十六进制 0x3C00）。硬件利用前两位（11）来指定段，但然后我们要处理偏移量 3KB。为了得到正确的反向偏移，我们必须从 3KB 中减去最 大的段地址：在这个例子中，段可以是 4KB，因此正确的偏移量是 3KB 减去 4KB，即−1KB。 只要用这个反向偏移量（−1KB）加上基址（28KB），就得到了正确的物理地址 27KB。用 户可以进行界限检查，确保反向偏移量的绝对值小于段的大小。



###	4.支持共享

随着分段机制的不断改进，系统设计人员很快意识到，通过再多一点的硬件支持，就 实现新的效率提升。具体来说，要节省内存，有时候在地址空间之间共享（share）某些内存段是有用的。尤其是，代码共享很常见，今天的系统仍然在使用。

为了支持共享，需要一些额外的硬件支持，这就是保护位（protection bit）。基本为每个段增加了几个位，标识程序是否能够读写该段，或执行其中的代码。通过将代码段标记为只读，同样的代码可以被多个进程共享，而不用担心破坏隔离。虽然每个进程都认为自己独占 这块内存，但操作系统秘密地共享了内存，进程不能修改这些内存，所以假象得以保持。 表 16.3 展示了一个例子，是硬件（和操作系统）记录的额外信息。可以看到，代码段 的权限是可读和可执行，因此物理内存中的一个段可以映射到多个虚拟地址空间。

![image-20230424203839876](/images/mdpic/image-20230424203839876.png)

有了保护位，前面描述的硬件算法也必须改变。除了检查虚拟地址是否越界，硬件还需要检查特定访问是否允许。如果用户进程试图写入只读段，或从非执行段执行指令，硬 件会触发异常，让操作系统来处理出错进程。



####	5.细粒度与粗粒度的分段

到目前为止，我们的例子大多针对只有很少的几个段的系统（即代码、栈、堆）。我们 可以认为这种分段是***粗粒度的（coarse-grained）***，因为它将地址空间分成较大的、粗粒度的块。但是，一些早期系统（如 Multics）更灵活，允许将地址空间划分为大量较小的段，这被称为***细粒度（fine-grained）***分段。 支持许多段需要进一步的硬件支持，并在内存中保存某种***段表（segment table）***。这种段表通常支持创建非常多的段，因此系统使用段的方式，可以比之前讨论的方式更灵活。例如， 像 Burroughs B5000 这样的早期机器可以支持成千上万的段，有了操作系统和硬件的支持，编译器可以将代码段和数据段划分为许多不同的部分。当时的考虑是，通过更细粒度的段，操作系统可以更好地了解哪些段在使用哪些没有，从而可以更高效地利用内存。



####	6.操作系统支持

现在你应该大致了解了分段的基本原理。系统运行时，地址空间中的不同段被重定位到物理内存中。与我们之前介绍的整个地址空间只有一个基址/界限寄存器对的方式相比，大量节省了物理内存。具体来说，栈和堆之间没有使用的区域就不需要再分配物理内存， 让我们能将更多地址空间放进物理内存。 然而，分段也带来了一些新的问题。我们先介绍必须关注的操作系统新问题。

第一个是老问题：操作系统在上下文切换时应该做什么？

你可能已经猜到了：各个段寄存器中的内容必须保存和恢复。显然，每个进程都有自己独立的虚拟地址空间，操作系统必须在进程运行前，确保这些寄存器被正确地赋值。

第二个问题更重要，即管理物理内存的空闲空间。新的地址空间被创建时，操作系统需要在物理内存中为它的段找到空间。之前，我们假设所有的地址空间大小相同，物理内存可以被认为是一些槽块，进程可以放进去。现在，每个进程都有一些段，每个段的大小也可能不同。 一般会遇到的问题是，物理内存很快充满了许多空闲空间的小洞，因而很难分配给新的段，或扩大已有的段。这种问题被称为***外部碎片（external fragmentation）***，如图 16.3 （左边）所示。

![image-20230424204448197](/images/mdpic/image-20230424204448197.png)

在这个例子中，一个进程需要分配一个 20KB 的段。当前有 24KB 空闲，但并不连续（是3个不相邻的块）。因此，操作系统无法满足这个 20KB 的请求。 

该问题的一种解决方案是紧凑（compact）物理内存，重新安排原有的段。例如，操作系统先终止运行的进程，将它们的数据复制到连续的内存区域中去，改变它们的段寄存器中的值，指向新的物理地址，从而得到了足够大的连续空闲空间。这样做，操作系统能让新的内存分配请求成功。但是，内存紧凑成本很高，因为拷贝段是内存密集型的，一般会占用大量的处理器时间。图 16.3（右边）是紧凑后的物理内存。 

一种更简单的做法是利用**空闲列表管理算法**，试图保留大的内存块用于分配。相关的算法可能有成百上千种，**包括传统的最优匹配（best-fit，从空闲链表中找最接近需要分配空 间的空闲块返回）、最坏匹配（worst-fit）、首次匹配（first-fit）以及像伙伴算法（buddy algorithm）** 这样更复杂的算法。Wilson 等人做过一个很好的调查，如果你想对这些算法了解更多，可以从它开始，或者等到第 17 章，我们将介绍一些基本知识。但遗憾的是，无论算法多么精妙，都无法完全消除外部碎片，因此，优秀的算法也只是试图减小它。

![image-20230424204926200](/images/mdpic/image-20230424204926200.png)

####	7.小结

分段解决了一些问题，帮助我们实现了更高效的虚拟内存。不只是动态重定位，通过避免地址空间的逻辑段之间的大量潜在的内存浪费，分段能更好地支持稀疏地址空间。它还很快，因为分段要求的算法很容易，很适合硬件完成，地址转换的开销极小。分段还有个附加的好处：代码共享。如果代码放在独立的段中，这样的段就可能被多个运行的程序共享。

但我们已经知道，在内存中分配不同大小的段会导致一些问题，我们希望克服。首先， 是我们上面讨论的**外部碎片**。由于段的大小不同，空闲内存被割裂成各种奇怪的大小，因此满足内存分配请求可能会很难。用户可以尝试采用聪明的算法，或定期紧凑内存， 但问题很根本，难以避免。 **第二个问题也许更重要，分段还是不足以支持更一般化的稀疏地址空间**。例如，如果 有一个很大但是稀疏的堆，都在一个逻辑段中，整个堆仍然必须完整地加载到内存中。换言之，如果使用地址空间的方式不能很好地匹配底层分段的设计目标，分段就不能很好地工作。因此我们需要找到新的解决方案。你准备好了吗？



##	17 空闲空间管理

本章暂且将对虚拟内存的讨论放在一边，来讨论所有内存管理系统的一个基本方面，无论是malloc库（管理进程中堆的页），还是操作系统本身（管理进程的地址空间）。具体来说，我们会讨论***空闲空间管理（free-space management）***的一些问题。

让问题更明确一点。管理空闲空间当然可以很容易，我们会在讨论分页概念时看到。 如果需要管理的空间被划分为固定大小的单元，就很容易。在这种情况下，只需要维护这些大小固定的单元的列表，如果有请求，就返回列表中的第一项。

如果要管理的空闲空间由大小不同的单元构成，管理就变得困难（而且有趣）。这种情况出现在用户级的内存分配库（如 malloc()和 free()），或者操作系统用分段（segmentation） 的方式实现虚拟内存。在这两种情况下，出现了外部碎片（external fragmentation）的问题： 空闲空间被分割成不同大小的小块，成为碎片，后续的请求可能失败，因为没有一块足够大的连续空闲空间，即使这时总的空闲空间超出了请求的大小。

![image-20230424205938348](/images/mdpic/image-20230424205938348.png)

上面展示了该问题的一个例子。在这个例子中，全部可用空闲空间是 20 字节，但被切成 两个 10 字节大小的碎片，导致一个 15 字节的分配请求失败。所以本章需要解决的问题是：

![image-20230424205947824](/images/mdpic/image-20230424205947824.png)



####	1.假设

本章的大多数讨论，将聚焦于用户级内存分配库中分配程序的辉煌历史。我们引用了 Wilson 的出色调查。 我们假定基本的接口就像 malloc()和 free()提供的那样。具体来说，void * malloc(size t size)需要一个参数 size，它是应用程序请求的字节数。函数返回一个指针（没有具体的类型， 在 C 语言的术语中是 void 类型），指向这样大小（或较大一点）的一块空间。对应的函数 void free(void *ptr)函数接受一个指针，释放对应的内存块。请注意该接口的隐含意义，**在释放空间时，用户不需告知库这块空间的大小。因此，在只传入一个指针的情况下，库必须能够弄清楚这块内存的大小**。我们将在稍后介绍是如何得知的。

该库管理的空间由于历史原因被称为堆，**在堆上管理空闲空间的数据结构通常称为*空闲列表（free list）***。该结构包含了管理内存区域中所有空闲块的引用。当然，该数据结构不一定真的是列表，而只是某种可以追踪空闲空间的数据结构。 

进一步假设，我们主要关心的是**外部碎片（external fragmentation）**，如上所述。当然， 分配程序也可能有**内部碎片（internal fragmentation）**的问题。如果分配程序给出的内存块超 出请求的大小，在这种块中超出请求的空间（因此而未使用）就被认为是内部碎片（因为 浪费发生在已分配单元的内部），这是另一种形式的空间浪费。但是，简单起见，同时也因为它更有趣，这里主要讨论外部碎片。 我们还假设，内存一旦被分配给客户，就不可以被重定位到其他位置。例如，一个程序调用 malloc()，并获得一个指向堆中一块空间的指针，这块区域就“属于”这个程序了， 库不再能够移动，直到程序调用相应的 free()函数将它归还。因此，不可能进行紧凑 （compaction）空闲空间的操作，从而减少碎片。但是，操作系统层在实现分段（segmentation） 时，却可以通过紧凑来减少碎片（正如第 16 章讨论的那样）。 最后我们假设，分配程序所管理的是连续的一块字节区域。在一些情况下，分配程序 可以要求这块区域增长。例如，一个用户级的内存分配库在空间快用完时，可以向内核申 请增加堆空间（通过 sbrk 这样的系统调用），但是，简单起见，我们假设这块区域在整个生命周期内大小固定。



####	2.底层机制

在深入策略细节之前，我们先来介绍大多数分配程序采用的通用机制。首先，探讨空 间分割与合并的基本知识。其次，看看如何快速并相对轻松地追踪已分配的空间。最后， 讨论如何利用空闲区域的内部空间维护一个简单的列表，来追踪空闲和已分配的空间。

![image-20230424214121927](/images/mdpic/image-20230424214121927.png)

![image-20230424214134896](/images/mdpic/image-20230424214134896.png)

![image-20230424214147454](/images/mdpic/image-20230424214147454.png)

![image-20230424214201681](/images/mdpic/image-20230424214201681.png)

![image-20230424214216171](/images/mdpic/image-20230424214216171.png)

![image-20230424214234662](/images/mdpic/image-20230424214234662.png)

![image-20230424214247516](/images/mdpic/image-20230424214247516.png)


![image-20230424214247516](/images/mdpic/image-20230424214247516.png)














#	并发



##	25 关于并发的对话

想象一下桌子上有很多桃子，还有很多人想吃掉它；比如，每个食客首先在视觉上识别桃子，然后试图抓住并吃掉桃子。这会出现什么问题？

好像你可能会看到别人也看到的桃子。如何他们先拿到，当你伸出手时，就拿不到桃子了。

如果使用其他方法来解决这个问题。如排队，当你到达前面时，抓起桃子并继续前进。这表示你要做所有的工作。我们曾经让很多人同时抓起桃子，速度更快。但以这种方式，我们只是一次一个，这是正确的，但速度较慢。最好的方法是**既快速又正确**。

事实证明，存在某些类型的程序，我们称之为**多线程（multi-threaded）**应用程序。每个线程（thread）都像在这个程序中运行的独立代理程序，代表程序做事。但是这些线程访问内存，对于它们来说，每个内存节点就像一个桃子。如果我们不协调线程之间的内存访问，程序将无法按预期工作。 

但是为什么我们要在操作系统课上谈论这个问题？这不就是应用程序编程吗？

实际上有几个原因。首先，操作系统必须用**锁（lock）**和**条件变量（condition variable）**这样的原语，来支持多线程应用程序，我们很快会讨论。其次，操作系统本身是**第一个并发程序**——它必须非常小心地访问自己的内存，否则会发生许多奇怪而可怕的事情。真的，会变得非常可怕。



##	26. 并发：介绍

> 本章将介绍为单个运行进程提供的新抽象：线程（thread）。
>
> 经典观点是一个程序只有一个执行点（一个程序计算器（pc寄存器），用来存放要执行的指令），但多线程（multi-threaded）程序会有多个执行点。换一个角度来看，每个线程类似于独立的进程，只有一点区别：它们共享地址空间，从而能够访问相同的数据。

线程和进程：

- 线程的状态和进程状态非常类似。如果有两个线程运行在一个处理器上，从运行一个线程切换到另一个线程时，必定发生上***下文切换（context switch）***。与进程相比，线程之间的上下文切换有一点主要区别：地址空间保持不变（既不需要切换当前使用的页表）。

- 线程和进程之间的另一个主要区别在于栈。在简单的传统进程地址空间模型（单线程进程）中，只有一个栈，通常位于地址空间的底部。然而在多线程的进程中，每个线程独立运行，不是地址空间中只有一个栈，而是每个线程都有一个栈。

  ![image-20230416203324116](/images/mdpic/image-20230416203324116.png)



####	1.实例：线程创建

线程的创建有点像进行函数调用。然而，并不是首先执行函数然后返回给调用者，而是被调用的程序创建一个新的执行线程，它可以独立于调用者运行，可能在从创建者返回之前运行，但也许会晚得多。

很显然，线程使得程序运行变得复杂。有了并发，情况变得更糟。



####	2.为什么更糟糕：共享数据



```c
#include "thread.h"

#define N 100000000

long sum = 0;

void Tsum() {
  for (int i = 0; i < N; i++) {
    sum++;
  }
}

int main() {
  create(Tsum);
  create(Tsum);
  join();
  printf("sum = %ld\n", sum);
}
```

每一次运行的结果都不确定！为什么？



####	3.核心问题：不可控的调度

设想我们的两个线程之一（线程 1）进入这个代码区域，并且因此将要增加一个计数器。 它将counter 的值（假设它这时是 50）加载到它的寄存器 eax 中。因此，线程 1 的 eax = 50。 然后它向寄存器加 1，因此 eax = 51。

现在，一件不幸的事情发生了：时钟中断发生。因此，操作系统将当前正在运行的线程（它的程序计数器、寄存器，包括 eax 等）的状态保存到线程的 TCB。 

现在更糟的事发生了：线程 2 被选中运行，并进入同一段代码。它也执行了第一条指令，获取计数器的值并将其放入其 eax 中 [请记住：运行时每个线程都有自己的专用寄存器。 上下文切换代码将寄存器虚拟化（virtualized），保存并恢复它们的值]。此时 counter 的值仍为 50，因此线程 2 的 eax = 50。

假设线程 2 执行接下来的两条指令，将 eax 递增 1（因此 eax = 51），然后将 eax 的内容保存到 counter（地址 0x8049a1c）中。因此，全局变量 counter 现 在的值是 51。 最后，又发生一次上下文切换，线程 1 恢复运行。还记得它已经执行过 mov 和 add 指令，现在准备执行最后一条 mov 指令。回忆一下，eax=51。因此，最后的 mov 指令执行， 将值保存到内存，counter 再次被设置为 51。 简单来说，发生的情况是：增加 counter 的代码被执行两次，初始值为 50，但是结果为 51。这个程序的“正确”版本应该导致变量 counter 等于 52。



这里展示的情况称为***竞态条件（race condition）***：结果取决于代码的时间执行。由于运气不好（即在执行过程中发生的上下文切换），我们得到了错误的结果。事实上，可能每次 都会得到不同的结果。因此，我们称这个结果是不确定的（indeterminate），而不是确定的 （deterministic）计算（我们习惯于从计算机中得到）。不确定的计算不知道输出是什么，它 在不同运行中确实可能是不同的。 由于执行这段代码的多个线程可能导致竞争状态，因此我们将此段代码称为临界区 （critical section）。临界区是访问共享变量（或更一般地说，共享资源）的代码片段，一定不 能由多个线程同时执行。

![image-20230416214539135](/images/mdpic/image-20230416214539135.png)



我们真正想要的代码就是所谓的***互斥（mutual exclusion）***。**这个属性保证了如果一个线程在临界区内执行，其他线程将被阻止进入临界区。**

事实上，所有这些术语都是由 Edsger Dijkstra 创造的，他是该领域的先驱，并且因为这 项工作和其他工作而获得了图灵奖。请参阅他 1968 年关于“Cooperating Sequential Processes” 的文章，该文对这个问题给出了非常清晰的描述。在本书的这一部分，我们将多次看 到 Dijkstra 的名字。



####	4.原子性期望

解决这个问题的一种途径是拥有更强大的指令，单步就能完成要做的事，从而消除不 合时宜的中断的可能性。比如，如果有这样一条超级指令怎么样？ 

```bash
memory-add 0x8049a1c, $0x1 
```

假设这条指令将一个值添加到内存位置，并且硬件保证它以***原子方式（atomically）***执行。当指令执行时，它会像期望那样执行更新。它不能在指令中间中断，因为这正是我们从硬件获得的保证：发生中断时，指令根本没有运行，或者运行完成，没有中间状态。硬件也可以很漂亮，不是吗？ 在这里，原子方式的意思是“作为一个单元”，有时我们说“全部或没有”。我们希望以原子方式执行 3 个指令的序列： 

```bash
mov 0x8049a1c, %eax 

add $0x1, %eax 

mov %eax, 0x8049a1c 
```

我们说过，如果有一条指令来做到这一点，我们可以发出这条指令然后完事。但在一 般情况下，不会有这样的指令。设想我们要构建一个并发的 B 树，并希望更新它。我们真的希望硬件支持“B 树的原子性更新”指令吗？可能不会，至少理智的指令集不会。 因此，我们要做的是**要求硬件提供一些有用的指令**，可以在这些指令上构建一个通用的集合，即所谓的***同步原语（synchronization primitive）***。通过使用这些硬件同步原语，加上操作系统的一些帮助，我们将能够构建多线程代码，以**同步和受控的方式访问临界区，从而可靠地产生正确的结果**—— 尽管有并发执行的挑战。很棒，对吗？

![image-20230416215006819](/images/mdpic/image-20230416215006819.png)

关键问题：如何实现同步？

为了构建有用的同步原语，需要从硬件中获得哪些支持？需要从操作系统中获得什么支持？如何正确有效地构建这些原语？程序如何使用它们来获得期望的结果？



####	5.还有一个问题：等待另一个线程

本章提出了并发问题，就好像线程之间只有一种交互，即**访问共享变量**，因此需要为临界区支持原子性。事实证明，还有另一种常见的交互，即**一个线程在继续之前必须等待另一个线程完成某些操作**。例如，当进程执行磁盘 I/O 并进入睡眠状态时，会产生这种交互。 当 I/O 完成时，该进程需要从睡眠中唤醒，以便继续进行。 因此，在接下来的章节中，**我们不仅要研究如何构建对同步原语的支持来支持原子性， 还要研究支持在多线程程序中常见的睡眠/唤醒交互的机制**。如果现在不明白，没问题！当你阅读条件变量（condition variable）的章节时，很快就会发生。



####	6.小结：为什么操作系统课需要研究并发

在结束之前，你可能会有一个问题：为什么我们要在 OS 类中研究并发？一个词：**“历史”**。**操作系统是第一个并发程序**，许多技术都是在操作系统内部使用的。后来，**在多线程的进程中，应用程序员也必须考虑这些事情**。 例如，设想有两个进程在运行。假设它们都调用 write()来写入文件，并且都希望将数据追加到文件中（即将数据添加到文件的末尾，从而增加文件的长度）。为此，这两个进程都 必须分配一个新块，记录在该块所在文件的 inode 中，并更改文件的大小以反映新的、增加的大小（插一句，在本书的第 3 部分，我们将更多地了解文件）。因为中断可能随时发生，所以更新这些共享结构的代码（例如，分配的位图或文件的 inode）是临界区。因此，从引 入中断的一开始，OS 设计人员就不得不担心操作系统如何更新内部结构。不合时宜的中断 会导致上述所有问题。毫不奇怪，页表、进程列表、文件系统结构以及几乎每个内核数据 结构都必须小心地访问，并使用正确的同步原语才能正常工作。

![image-20230416215746071](/images/mdpic/image-20230416215746071.png)



##	27 插叙：线程API



####	1.线程创建

编写多线程程序的第一步就是创建新线程，因此必须存在某种线程创建接口。在 POSIX 中，很简单：

```c
#include <pthread.h>
int
pthread_create( pthread_t *thread,const pthread_attr_t *attr,
 				void * (*start_routine)(void*),void * arg); 
```



####	2.线程完成

等待线程的完成（使用`pthread_join`函数）

```c
#include <pthread.h>
int pthread_join(pthread_t thread, void **retval);
```



####	3.锁



```bash
#include <pthread.h>

pthread_mutex_t fastmutex = PTHREAD_MUTEX_INITIALIZER;

pthread_mutex_t recmutex = PTHREAD_RECURSIVE_MUTEX_INITIALIZER_NP;

pthread_mutex_t errchkmutex = PTHREAD_ERRORCHECK_MUTEX_INITIALIZER_NP;

int pthread_mutex_init(pthread_mutex_t *mutex, const pthread_mutexattr_t 							*mutexattr);

int pthread_mutex_lock(pthread_mutex_t *mutex);

int pthread_mutex_trylock(pthread_mutex_t *mutex);

int pthread_mutex_unlock(pthread_mutex_t *mutex);

int pthread_mutex_destroy(pthread_mutex_t *mutex);

```





####	4.条件变量

```bash
#include <pthread.h>

pthread_cond_t cond = PTHREAD_COND_INITIALIZER;

int pthread_cond_init(pthread_cond_t *cond, pthread_condattr_t *cond_attr);

int pthread_cond_signal(pthread_cond_t *cond);

int pthread_cond_broadcast(pthread_cond_t *cond);

int pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex);

int pthread_cond_timedwait(pthread_cond_t *cond, pthread_mutex_t *mutex, const 								struct timespec *abstime);

int pthread_cond_destroy(pthread_cond_t *cond);
```



####	5.小结

我们介绍了基本的 pthread 库，包括线程创建，通过锁创建互斥执行，通过条件变量的信号和等待。要想写出健壮高效的多线程代码，只需要耐心和万分小心！ 

本章结尾我们给出编写一些多线程代码的建议（参见补充内容）。API 的其他方面也很 有趣。如果需要更多信息，请在 Linux 系统上输入 man -k pthread，查看构成整个接口的超 过一百个 API。

但是，这里讨论的基础知识应该让你能够构建复杂的（并且希望是正确的和高性能的）多线程程序。线程难的部分不是 API，而是如何构建并发程序的棘手逻辑。请继续阅读以了解更多信息。

![image-20230416221241470](/images/mdpic/image-20230416221241470.png)



##	28 锁

通过对并发的介绍，我们看到了并发编程的一个最基本问题：我们希望原子执行一系列指令，但由于单处理上的中断，或多个线程在多处理器上并发执行，导致我们做不到。

本章介绍了锁（lock），直接解决这一问题。程序员在源代码中加锁，放在临界区周围，保证临界区能够像单条原子指令一样执行。



####	1.锁的基本思想

举个例子，假设临界区像这样，典型的更新共享变量：

```c
balance = balance + 1;
```

当然，其他临界区也是可能的，比如为链表增加一个元素，或对共享结构的复杂更新 操作。为了使用锁，我们给临界区增加了这样一些代码：

```c
lock_t mutex;	// some globally-allocated lock 'mutex'
...
lock(&mutex);
balance = balance + 1;
unlock(&mutex);
```

锁就是一个变量，因此我们需要声明一个某种类型的锁变量（lock variable，如上面的 mutex），才能使用。这个锁变量（简称锁）保存了锁在某一时刻的状态。它要么是可用的 ，表示没有线程持有锁，要么是被占用的，表示有一个线程持有锁，正处于临界区。我们也可以保存其他的信息，比如持有锁的线程，或请求获取锁的线程队列，但这些信息会隐藏起来，锁的使用者不会发现。

lock()和 unlock()函数的语义很简单。调用 lock()尝试获取锁，如果没有其他线程持有锁 （即它是可用的），该线程会获得锁，进入临界区。这个线程有时被称为锁的持有者（owner）。 如果另外一个线程对相同的锁变量（本例中的 mutex）调用 lock()，因为锁被另一线程持有， 该调用不会返回。这样，当持有锁的线程在临界区时，其他线程就无法进入临界区。 锁的持有者一旦调用 unlock()，锁就变成可用了。如果没有其他等待线程（即没有其他 线程调用过 lock()并卡在那里），锁的状态就变成可用了。如果有等待线程（卡在 lock()里）， 其中一个会（最终）注意到（或收到通知）锁状态的变化，获取该锁，进入临界区。 

锁为程序员提供了最小程度的调度控制。**我们把线程视为程序员创建的实体，但是被操作系统调度**，具体方式由操作系统选择。**锁让程序员获得一些控制权**。**通过给临界区加锁，可以保证临界区内只有一个线程活跃。锁将原本由操作系统调度的混乱状态变得更为可控。**



####	2.Pthread锁

POSIX 库将锁称为***互斥量（mutex）***，因为它被用来提供线程之间的互斥。即当一个线程在临界区，它能够阻止其他线程进入直到本线程离开临界区。因此，如果你看到下面的 POSIX 线程代码，应该理解它和上面的代码段执行相同的任务（我们再次使用了包装函数 来检查获取锁和释放锁时的错误）。

```c
pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;

pthread_mutex_lock(&lock,NULL);
balance = balance + 1;
pthread_mutex_unlock(&lock,NULL);
```

你可能还会注意到，POSIX 的 lock 和 unlock 函数会传入一个变量，因为我们可能用不同的锁来保护不同的变量。这样可以增加并发：不同于任何临界区都使用同一个大锁（粗粒度的锁策略），通常大家会用不同的锁保护不同的数据和结构，从而允许更多的线程进入临界区（细粒度的方案）。



####	3.实现一个锁

如何实现一个锁呢？我们需要什么硬件支持？需要什么操作系统的支持？



我们需要硬件和操作系统的帮助来实现一个可用的锁。近些年来，各种计算机体系结构的指令集都增加了一些不同的硬件原语，我们不研究这些指令是如何实现的（毕竟，这是计算机体系结构课程的主题），只研究如何使用它们来实现像锁这样的互斥原语。我们也会研究操作系统如何发展完善，支持实现成熟复杂的锁库。



####	4.锁的度量

在实现锁之前，我们应该首先明确目标，因此我们要问，如何评价一种锁实现的效果。 为了评价锁是否能工作（并工作得好），我们应该先设立一些标准。

第一是锁是否能完成它的基本任务，即提供***互斥（mutual exclusion）***。最基本的，锁是否有效，能够阻止多个线程进入临界区？

第二是***公平性（fairness）***。当锁可用时，是否每一个竞争线程有公平的机会抢到锁？用另一个方式来看这个问题是检查更极端的情况：是否有竞争锁的线程会饿死（starve），一直 无法获得锁？

最后是***性能（performance）***，具体来说，是使用锁之后增加的时间开销。有几种场景需要考虑。一种是没有竞争的情况，即只有一个线程抢锁、释放锁的开支如何？另外一种是一个 CPU 上多个线程竞争，性能如何？最后一种是多个 CPU、多个线程竞争时的性能。通过比较不同的场景，我们能够更好地理解不同的锁技术对性能的影响，下面会进行介绍



####	5.控制中断

最早提供的互斥解决方案之一，就是在临界区关闭中断。这个解决方案是为单处理器系统开发的。代码如下：

```c
void lock() {
DisableInterrupts();
}
void unlock() {
EnableInterrupts();
} 
```

假设我们运行在这样一个单处理器系统上。通过在进入临界区之前关闭中断（使用特殊的硬件指令），可以保证临界区的代码不会被中断，从而**原子地执行**。结束之后，我们重新打开中断（同样通过硬件指令），程序正常运行。 这个方法的主要优点就是简单。显然不需要费力思考就能弄清楚它为什么能工作。没有中断，线程可以确信它的代码会继续执行下去，不会被其他线程干扰。 

遗憾的是，缺点很多。首先，这种方法要求我们允许所有调用线程执行特权操作（打开关闭中断），即信任这种机制不会被滥用。众所周知，如果我们必须信任任意一个程序， 可能就有麻烦了。这里，麻烦表现为多种形式：

第一，一个**贪婪**的程序可能在它开始时就调用 lock()，从而独占处理器。更糟的情况是，恶意程序调用 lock()后，一直死循环。后一 种情况，系统无法重新获得控制，只能重启系统。关闭中断对应用要求太多，不太适合作 为通用的同步解决方案。 

第二，**这种方案不支持多处理器**。如果多个线程运行在不同的 CPU 上，每个线程都试图进入同一个临界区，关闭中断也没有作用。线程可以运行在其他处理器上，因此能够进 入临界区。多处理器已经很普遍了，我们的通用解决方案需要更好一些。 

第三，**关闭中断导致中断丢失，可能会导致严重的系统问题**。假如磁盘设备完成了读取请求，但 CPU 错失了这一事实，那么，操作系统如何知道去唤醒等待读取的进程？ 

最后一个不太重要的原因就是**效率低**。与正常指令执行相比，现代 CPU 对于关闭和打开中断的代码执行得较慢。 基于以上原因，只在很有限的情况下用关闭中断来实现互斥原语。例如，在某些情况 下操作系统本身会采用屏蔽中断的方式，保证访问自己数据结构的原子性，或至少避免某些复杂的中断处理情况。这种用法是可行的，因为在操作系统内部不存在信任问题，它总是信任自己可以执行特权操作。

![image-20230417215934330](/images/mdpic/image-20230417215934330.png)



####	6.测试并设置指令（atomic exchange）

因为关闭中断的方法无法工作在多处理器上，所以系统设计者开始让硬件支持锁。最早的多处理器系统，像 20 世纪 60 年代早期的 Burroughs B5000，已经有这些支持。今天所有的系统都支持，甚至包括单 CPU 的系统。 

最简单的硬件支持是***测试并设置指令（test-and-set instruction）***，也叫作***原子交换（atomic exchange）***。为了理解 test-and-set 如何工作，我们首先实现一个不依赖它的锁，用一个变量标记锁是否被持有。 

在第一次尝试中，想法很简单：用一个变量来标志锁是否被某些线程占用。 第一个线程进入临界区，调用 lock()，检查标志是否为 1（这里不是 1），然后设置标志为 1， 表明线程持有该锁。结束临界区时，线程调用 unlock()，清除标志，表示锁未被持有。

```c
typedef struct lock_t { int flag; } lock_t;

void init(lock_t *mutex){
    mutex->flag = 0;
}
void lock(lock_t *mutex){
    while(mutex->flag == 1);		// TEST the flag  
    mutex->flag = 1;				// now SET it!
}
void unlock(lock_t *mutex){
    mutex->flag  = 0;
}

```

当第一个线程正处于临界区时，如果另一个线程调用 lock()，它会在while循环中自***旋等待（spin-wait)***，直到第一个线程调用 unlock()清空标志。然后等待的线程会退出while循环，设置标志，执行临界区代码。 

遗憾的是，这段代码有两个问题：正确性和性能。这个正确性问题在并发编程中很常见。假设代码按照表 28.1 执行，开始时 flag=0。

![image-20230417220829846](/images/mdpic/image-20230417220829846.png)

从这种交替执行可以看出，通过适时的（不合时宜的？）中断，我们很容易构造出两个线程都将标志设置为 1，都能进入临界区的场景。

这种行为就是专家所说的“不好”，我们显然没有满足最基本的要求：互斥。 性能问题（稍后会有更多讨论）主要是线程在等待已经被持有的锁时，采用了***自旋等待（spin-waiting）***的技术，就是不停地检查标志的值。自旋等待在等待其他线程释放锁的时候会浪费时间。尤其是在单处理器上，一个等待线程等待的目标线程甚至无法运行（至少在上下文切换之前）！我们要开发出更成熟的解决方案，也应该考虑避免这种浪费。



####	7.实现可用的自旋锁

尽管上面例子的想法很好，**但没有硬件的支持是无法实现的**。幸运的是，一些系统提供了这一指令，支持基于这种概念创建简单的锁。

这个更强大的指令有不同的名字：

在 SPARC 上，这个指令叫 ldstub（load/store unsigned byte，加载/保存无符号字节）；

**在 x86 上，是 xchg （atomic exchange，原子交换）指令。**

但它们基本上在不同的平台上做同样的事，通常称为***测试并设置指令（test-and-set）***。我们用如下的 C 代码片段来定义测试并设置指令做了什么：

```c
int TestAndSet(int *old_ptr, int new){
    int old = *old_ptr;
    *old_ptr = new;
    return old;
}
```

测试并设置指令做了下述事情。它返回 old_ptr 指向的旧值，同时更新为 new 的新值。 当然，关键是这些代码是**原子地（atomically）**执行。因为既可以测试旧值，又可以设置新值，所以我们把这条指令叫作“测试并设置”。这一条指令完全可以实现一个简单的***自旋锁（spin lock）***，如图 28.2 所示。或者你可以先尝试自己实现，这样更好！

我们来确保理解为什么这个锁能工作。首先假设一个线程在运行，调用 lock()，没有其他线程持有锁，所以 flag 是 0。当调用 TestAndSet(flag, 1)方法，返回 0，线程会跳出 while 循环，获取锁。同时也会原子的设置 flag 为 1，标志锁已经被持有。当线程离开临界区，调 用 unlock()将 flag 清理为 0。

```c
typedef struct lock_t{
    int flag;
}lock_t;

void init(lock_t *lock){
    lock->flag = 0;
}

void lock(lock_t *lock){
    while(TestAndSet(&lock->flag, 1) == 1);		// spin-with (do nothing)
}

void unlock(lock_t *lock){
    lock->flag = 0;
}
```

第二种场景是，当某一个线程已经持有锁（即 flag 为 1）。

本线程调用 lock()，然后调用 TestAndSet(flag, 1)，这一次返回 1。只要另一个线程一直持有锁，TestAndSet()会重复返回 1， 本线程会一直自旋。

当 flag 终于被改为 0，本线程会调用 TestAndSet()，返回 0 并且原子地设置为 1，从而获得锁，进入临界区。

将测试（test旧的锁值）和设置（set 新的值）合并为一个原子操作之后，我们保证了只有一个线程能获取锁。这就实现了一个有效的**互斥原语**！ 

你现在可能也理解了为什么这种锁被称为***自旋锁（spin lock）***。这是最简单的一种锁，一直自旋，利用 CPU 周期，直到锁可用。在单处理器上，需要抢占式的调度器（preemptive scheduler，即不断通过时钟中断一个线程，运行其他线程）。否则，自旋锁在单 CPU 上无法使用，因为一个自旋的线程永远不会放弃 CPU。



####	8.评估自旋锁

现在可以按照之前的标准来评价基本的自旋锁了。

锁最重要的一点是正确性（correctness）： 能够互斥吗？答案是可以的：自旋锁一次只允许一个线程进入临界区。因此，这是正确的锁。 

下一个标准是公平性（fairness）。自旋锁对于等待线程的公平性如何呢？能够保证一个等待线程会进入临界区吗？答案是自旋锁不提供任何公平性保证。实际上，自旋的线程在竞争条件下可能会永远自旋。自旋锁没有公平性，可能会导致饿死。 

最后一个标准是性能（performance）。使用自旋锁的成本是多少？为了更小心地分析，我们建议考虑几种不同的情况。首先，考虑线程在单处理器上竞争锁的情况。然后，考虑这些线程跨多个处理器。 对于自旋锁，在单 CPU 的情况下，性能开销相当大。假设一个线程持有锁进入临界区 时被抢占。调度器可能会运行其他每一个线程（假设有 N−1 个这种线程）。而其他线程都在竞争锁，都会在放弃 CPU 之前，自旋一个时间片，浪费 CPU 周期。 但是，在多 CPU 上，自旋锁性能不错（如果线程数大致等于 CPU 数）。假设线程 A 在 CPU 1，线程 B 在 CPU 2 竞争同一个锁。线程 A（CPU 1）占有锁时，线程 B 竞争锁就会自 旋（在 CPU 2 上）。然而，临界区一般都很短，因此很快锁就可用，然后线程 B 获得锁。自旋等待其他处理器上的锁，并没有浪费很多 CPU 周期，因此效果不错。



####	9.比较并交换

某些系统提供了另一个硬件原语，即比较并交换指令（SPARC 系统中是 compare-and-swap， **x86 系统是 compare-and-exchange**）。图 28.3 是这条指令的 C语言伪代码。

```c
int CompareAndSwap(int *ptr, int expected, int new){
    int actual = *ptr;
    if(actual == expected)
        *ptr = new;
    return actual;
}
```

比较并交换的基本思路是检测 ptr 指向的值是否和 expected 相等；如果是，更新 ptr所指的值为新值。否则，什么也不做。

不论哪种情况，都返回该内存地址的实际值，让调用者能够知道执行是否成功。 有了比较并交换指令，就可以实现一个锁，类似于用测试并设置那样。例如，我们只要用下面的代码替换 lock()函数：

```c
void lock(lock_t *lock){
    while(CompareAndSwap(&lock->flag, 0, 1) == 1);		// spin
}
```

其余代码和上面测试并设置的例子完全一样。代码工作的方式很类似，检查标志是否为 0，如果是，原子地交换为 1，从而获得锁。锁被持有时，竞争锁的线程会自旋。



最后，你可能会发现，比较并交换指令比测试并设置更强大。当我们在将来简单探讨***无等待同步（wait-free synchronization）***时，会用到这条指令的强大之处。然而，如果只用它实现一个简单的自旋锁，它的行为等价于上面分析的自旋锁。





####	10.链接的加载和条件式存储指令







####	11.获取并增加

最后一个硬件原语是***获取并增加（fetch-and-add）***指令，它能原子地返回特定地址的旧值，并且让该值自增一。获取并增加的 C 语言伪代码如下：

在这个例子中，我们会用获取并增加指令，实现一个更有趣的 ***ticket 锁***，这是 Mellor-Crummey 和 Michael Scott。



####	12.自旋过多：怎么办

基于硬件的锁简单（只有几行代码）而且有效（如果高兴，你甚至可以写一些代码来验证），这也是任何好的系统或者代码的特点。但是，某些场景下，这些解决方案会效率低 下。以两个线程运行在单处理器上为例，当一个线程（线程 0）持有锁时，被中断。第二个 线程（线程 1）去获取锁，发现锁已经被持有。因此，它就开始自旋。接着自旋。 然后它继续自旋。最后，时钟中断产生，线程 0 重新运行，它释放锁。最后（比如下次它运行时），线程 1 不需要继续自旋了，它获取了锁。因此，类似的场景下，一个线程会 一直自旋检查一个不会改变的值，浪费掉整个时间片！如果有 N 个线程去竞争一个锁，情况会更糟糕。同样的场景下，会浪费 N−1 个时间片，只是自旋并等待一个线程释放该锁。 因此，我们的下一个问题是：

**如何让锁不进行不必要地自旋，浪费CPU时间？**

只有硬件支持是不够的。我们还需要操作系统支持！接下来看一看怎么解决这一问题。



####	13.简单方法：让出来吧

硬件支持让我们有了很大的进展：我们已经实现了**有效、公平（通过 ticket 锁）的锁**。 但是，问题仍然存在：如果临界区的线程发生上下文切换，其他线程只能一直自旋，等待被中断的（持有锁的）进程重新运行。有什么好办法？ 第一种简单友好的方法就是，在要自旋的时候，放弃 CPU。

```c
void init(){
    flag = 0;
}
void lock(){
    while(TestAndSet(&flag,1) == 1)
        yield();		// give up the cpu
}
void unlock(){
    flag = 0;
}
```

在这种方法中，我们假定操作系统提供原语 yield()，线程可以调用它主动放弃 CPU， 让其他线程运行。线程可以处于 3 种状态之一（运行、就绪和阻塞）。yield()系统调用能够让运行（running）态变为就绪（ready）态，从而允许其他线程运行。因此，让出线程本质上取消调度（deschedules）了它自己。 考虑在单 CPU 上运行两个线程。在这个例子中，基于 yield 的方法十分有效。一个线程 调用 lock()，发现锁被占用时，让出 CPU，另外一个线程运行，完成临界区。在这个简单的例子中，让出方法工作得非常好。 

现在来考虑许多线程（例如 100 个）反复竞争一把锁的情况。在这种情况下，一个线程持有锁，在释放锁之前被抢占，其他 99 个线程分别调用 lock()，发现锁被抢占，然后让出 CPU。假定采用某种轮转调度程序，这 99 个线程会一直处于运行—让出这种模式，直到持有锁的线程再次运行。虽然比原来的浪费 99 个时间片的自旋方案要好，但这种方法仍然成本很高，上下文切换的成本是实实在在的，因此浪费很大。 更糟的是，我们还没有考虑饿死的问题。一个线程可能一直处于让出的循环，而其他线程反复进出临界区。很显然，我们需要一种方法来解决这个问题。



####	14.使用队列：休眠替代自旋

前面一些方法的真正问题是存在太多的偶然性。调度程序决定如何调度。如果调度不合理，线程或者一直自旋（第一种方法），或者立刻让出 CPU（第二种方法）。无论哪种方法，都可能造成浪费，也能导致饿死。 因此，我们必须显式地施加某种控制，决定锁释放时，谁能抢到锁。为了做到这一点， 我们需要操作系统的更多支持，并需要一个队列来保存等待锁的线程。 

简单起见，我们利用 Solaris 提供的支持，它提供了两个调用：park()能够让调用线程休眠，unpark(threadID)则会唤醒 threadID 标识的线程。可以用这两个调用来实现锁，让调用者在获取不到锁时睡眠，在锁可用时被唤醒。

```c
typedef struct lock_t{
    int flag;
    int gurad;
    queue_t *q;
}lock_t;

void lock_init(lock_t *m){
    m->flag = 0;
    m->guard = 0;
    queue_init(m->q);
}

void lock(lock_t *m){
    while(TestAndSet(&m->guard,1) == 1);	// acquire gurad lock by spining
    if(m->flag == 0){			// 无锁
        m->flag = 1;    
    }else{						// 有锁，加入队列并休眠。
        queue_add(m->q,gettid());
        park();					// thread sleep
    }
    m->guard = 0;
}

void unlock(lock_t *m){
    while(TestAndSet(&m->guard, 1) == 1);
    if(queue_empty(m->q))	m->flag = 0;
    else	unpark(queue_remove(m->q));	
    m->guard = 0;
}
```






####	15.不同操作系统，不同实现

目前我们看到，为了构建更有效率的锁，一个操作系统提供的一种支持。其他操作系统也提供了类似的支持，但细节不同。 例如，Linux 提供了 **futex**，它类似于 Solaris 的接口，但提供了更多内核功能。

具体来说， 每个 futex 都关联一个特定的物理内存位置，也有一个事先建好的内核队列。调用者通过 futex 调用（见下面的描述）来睡眠或者唤醒。 具体来说，有两个调用。调用 futex_wait(address, expected)时，如果 address 处的值等于 expected，就会让调线程睡眠。否则，调用立刻返回。调用 futex_wake(address)唤醒等待队列中的一个线程。



####	16.两阶段锁

最后一点：Linux 采用的是一种古老的锁方案，多年来不断被采用，可以追溯到 20 世 纪 60 年代早期的 Dahm 锁，现在也称为***两阶段锁（two-phase lock）***。

两阶段锁意识到自旋可能很有用，尤其是在很快就要释放锁的场景。因此，两阶段锁的第一阶段会先自旋一段时间，希望它可以获取锁。 但是，如果第一个自旋阶段没有获得锁，第二阶段调用者会睡眠，直到锁可用。上文的 Linux 锁就是这种锁，不过只自旋一次；更常见的方式是在循环中自旋固定的次数，然后 使用 futex 睡眠。 两阶段锁是又一个杂合（hybrid）方案的例子，即结合两种好想法得到更好的想法。当 然，硬件环境、线程数、其他负载等这些因素，都会影响锁的效果。事情总是这样，让单个通用目标的锁，在所有可能的场景下都很好，这是巨大的挑战。



####	17.小结

以上的方法展示了如今真实的锁是如何实现的：一些硬件支持（更加强大的指令）和 一些操作系统支持（例如 Solaris 的 park()和 unpark()原语，Linux 的 futex）。当然，细节有 所不同，执行这些锁操作的代码通常是高度优化的。读者可以查看 Solaris 或者 Linux 的代码以了解更多信息。David 等人关于现代多处理器的锁策略的对比也值得一看





## 29 基于锁的并发数据结构

在结束锁的讨论之前，我们先讨论如何在常见的数据结构中使用锁。通过锁可以使输数据结构线程安全（thread safe）。当然，具体如何加锁决定了该数据结构的正确性和效率。因此， 我们的挑战是：

![image-20230418223406572](/images/mdpic/image-20230418223406572.png)



####	1.并发计数器

这是最简单的数据结构，使用很少的代码便能实现。我们的问题是，如何让代码线程安全？

```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <pthread.h>

typedef struct counter_t{
    int val;
    pthread_mutex_t lock;
}counter_t;



void init(counter_t *c){
    c->val = 0;
    pthread_mutex_init(&c->lock,NULL);
}

void increment(counter_t *c){
    pthread_mutex_lock(&c->lock);
    c->val++;
    pthread_mutex_unlock(&c->lock);
}

void decrement(counter_t *c){
    pthread_mutex_lock(&c->lock);
    c->val--;
    pthread_mutex_unlock(&c->lock);
}

int get(counter_t *c){
    pthread_mutex_lock(&c->lock);
    int ret = c->val;
    pthread_mutex_unlock(&c->lock);
    return ret;
}

void* tfun(void* arg){
    counter_t* c = (counter_t*) arg;
    for(int i = 0; i < 100000; ++i)
        increment(c);
    return NULL;
}

int main(){
    counter_t my_ct;
    init(&my_ct);
    const int N = 64;
    pthread_t threads[N];

    for(int i = 0; i < N; ++i){
        pthread_create(&threads[i],NULL,tfun,&my_ct);
    }

    for(int i = 0; i < N; ++i)
        pthread_join(threads[i],NULL);

    int res = get(&my_ct);
    printf("%d\n", res);
    return 0;
}
```

这个并发计数器简单、正确。实际上，它遵循了最简单、最基本的并发数据结构中常见的数据模式：它只是加了一把锁，在调用函数操作该数据结构时获取锁，从调用返回时释放锁。

这种方式类似基于观察者（monitor）的数据结构，在调用、退出对象方法时，会自动获取锁、释放锁。 

现在，有了一个并发数据结构，问题可能就是性能了。如果这个结构导致运行速度太慢， 那么除了简单加锁，还需要进行优化。如果需要这种优化，那么本章的余下部分将进行探讨。 请注意，如果数据结构导致的运行速度不是太慢，那就没事！如果简单的方案就能工作，就不需要精巧的设计。

**简单但无法扩展**

为了理解简单方法的性能成本，我们运行一个基准测试，每个线程更新同一个共享计数器固定次数，然后我们改变线程数。图 29.3 给出了运行 1 个线程到 4 个线程的总耗时， 其中每个线程更新 100 万次计数器。本实验是在 4 核 Intel 2.7GHz i5 CPU 的 iMac 上运行。 通过增加 CPU，我们希望单位时间能够完成更多的任务。

从图 29.3 上方的曲线（标为“精确”）可以看 出，同步的计数器扩展性不好。单线程完成100万次更新只需要很短的时间（大约 0.03s），而两个线程并发执行，每个更新 100 万次，性能下降很多（超 过 5s！）。线程更多时，性能更差。 理想情况下，你会看到多处理上运行的多线程就像单线程一样快。达到这种状态称为完美扩展 （perfect scaling）。虽然总工作量增多，但是并行执行后，完成任务的时间并没有增加。

![image-20230418224354937](/images/mdpic/image-20230418224354937.png)

**可扩展的计数**

懒惰计数器（sloppy counter）

```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <pthread.h>

#define N 64

typedef struct counter_t{
    int             global;
    pthread_mutex_t glock;
    int             local[N];
    pthread_mutex_t llock[N];
    int             threshold;      // update frequency
}counter_t;

void init(counter_t *c,int k){
    c->threshold = k;
    c->global = 0;
    pthread_mutex_init(&c->glock,NULL);
    for(int i = 0; i < N; ++i){
        c->local[i] = 0;
        pthread_mutex_init(&c->llock[i],NULL);
    }
}

void update(counter_t *c,int threadID,int amt){
    int cpu = threadID % N;
    pthread_mutex_lock(&c->llock[cpu]);
    c->local[cpu] += amt;
    if(c->local[cpu] >= c->threshold){
        pthread_mutex_lock(&c->glock);
        c->global += c->local[cpu];
        c->local[cpu] = 0;
        pthread_mutex_unlock(&c->glock);
    }
    pthread_mutex_unlock(&c->llock[cpu]);
}

int get(counter_t *c){
    pthread_mutex_lock(&c->glock);
    int ret = c->global;
    pthread_mutex_unlock(&c->glock);
    return ret;
}


void* tfun(void* arg){
    counter_t* c = (counter_t*) arg;
    for(int i = 0; i < 100000; ++i)
        update(c,rand(),1);
    return NULL;
}


int main(){
    counter_t my_ct;
    init(&my_ct,100000);
    pthread_t threads[N];
    srand(time(0));

    for(int i = 0; i < N; ++i){
        pthread_create(&threads[i],NULL,tfun,&my_ct);
    }

    for(int i = 0; i < N; ++i)
        pthread_join(threads[i],NULL);

    int res = get(&my_ct);
    printf("%d\n", res);
    return 0;
}
```



图 29.4 展示了阈值 S 的重要性，在 4 个 CPU 上的 4 个线程，分别增加计数器100万次。 如果 S 小，性能很差（但是全局计数器精确度高）。如果S大，性能很好，但是全局计数器会有延时。**懒惰计数器就是在准确性和性能之间折中。**

![image-20230418224457411](/images/mdpic/image-20230418224457411.png)



####	2.并发链表

接下来看一个更复杂的数据结构，链表。同样，我们从一个基础实现开始。简单起见， 我们只关注链表的插入操作和查找：

```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <pthread.h>

#define N 8


typedef struct node_t{
    int val;
    struct node_t *next;
}node_t;

typedef struct list_t{
    node_t *head;
    pthread_mutex_t lock;
}list_t;

void List_Init(list_t *L){
    L->head = NULL;
    pthread_mutex_init(&L->lock,NULL);
}

void List_Insert(list_t *L, int key){
    node_t* newnode = (node_t*)malloc(sizeof(node_t));
    if(newnode == NULL){
        return;
    }
    pthread_mutex_lock(&L->lock);

    newnode->val = key;
    newnode->next = L->head;
    L->head = newnode;

    pthread_mutex_unlock(&L->lock);
}

int List_Lookup(list_t *L,int key){
    node_t *cur = L->head;
    int res = 0;
    pthread_mutex_lock(&L->lock);
    while(cur){
        if(cur->val == key){
            res = 1;
            break;
        }        
    }
    pthread_mutex_unlock(&L->lock);
    return res;
}


void* tfun(void* arg){
    list_t *L = (list_t*)arg;
    List_Insert(L,111);
    return NULL;
}


int main(){

    list_t *L = (list_t*)malloc(sizeof(list_t));
    List_Init(L);
    
    pthread_t threads[N];

    for(int i = 0; i < N; ++i)
        pthread_create(&threads[i],NULL,tfun,L);
    
    for(int i = 0; i < N; ++i)
        pthread_join(threads[i],NULL);
    
    node_t *cur = L->head;
    while(cur){
        printf("%d->", cur->val);
        cur = cur->next;
    }
    printf("NULL\n");
    return 0;
}
```

从代码中可以看出，代码插入函数入口处获取锁，结束时释放锁。如果 malloc 失败（在 极少的时候），会有一点小问题，在这种情况下，代码在插入失败之前，必须释放锁。 

事实表明，这种异常控制流容易产生错误。最近一个 Linux 内核补丁的研究表明，有 40%都是这种很少发生的代码路径（实际上，这个发现启发了我们自己的一些研究，我们从 Linux 文件系统中移除了所有内存失败的路径，得到了更健壮的系统）。因此，挑战来了：我们能够重写插入和查找函数，保持并发插入正确，但避免在失败 情况下也需要调用释放锁吗？ 

在这个例子中，答案是可以。具体来说，我们调整代码，让获取锁和释放锁只环绕插入代码的真正临界区。前面的方法有效是因为部分工作实际上不需要锁，假定 malloc()是线 程安全的，每个线程都可以调用它，不需要担心竞争条件和其他并发缺陷。只有在更新共 享列表时需要持有锁。图 29.7 展示了这些修改的细节。 对于查找函数，进行了简单的代码调整，跳出主查找循环，到单一的返回路径。这样做减少了代码中需要获取锁、释放锁的地方，降低了代码中不小心引入缺陷（诸如在返回前忘记释放锁）的可能性。

**扩展链表**

尽管我们有了基本的并发链表，但又遇到了这个链表扩展性不好的问题。研究人员发 现的增加链表并发的技术中，有一种叫作***过手锁***（hand-over-hand locking，也叫作锁耦合， lock coupling）[MS04]。 原理也很简单。每个节点都有一个锁，替代之前整个链表一个锁。遍历链表的时候， 首先抢占下一个节点的锁，然后释放当前节点的锁。 从概念上说，过手锁链表有点道理，它增加了链表操作的并发程度。但是实际上，在 遍历的时候，每个节点获取锁、释放锁的开销巨大，很难比单锁的方法快。即使有大量的 线程和很大的链表，这种并发的方案也不一定会比单锁的方案快。也许某种杂合的方案（一定数量的节点用一个锁）值得去研究。

![image-20230418230037036](/images/mdpic/image-20230418230037036.png)

####	3.并发队列

你现在知道了，总有一个标准的方法来创建一个并发数据结构：**添加一把大锁**。对于一个队列，我们将跳过这种方法，假定你能弄明白。 我们来看看 Michael 和 Scott 设计的、更并发的队列：

```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <pthread.h>
#include <assert.h>

#define N 8

typedef struct node_t{
    int val;
    struct node_t *next;
}node_t;

typedef struct queue_t{
    node_t *front;
    node_t *rear;
    pthread_mutex_t frontlock;				//头尾锁
    pthread_mutex_t rearlock;
}queue_t;

void Queue_Init(queue_t *q){
    node_t *dummy = (node_t*)malloc(sizeof(node_t));	//虚拟节点，方便删除
    dummy->next = NULL;
    dummy->val = 0;
    q->front = q->rear = dummy;
    pthread_mutex_init(&q->frontlock,NULL);
    pthread_mutex_init(&q->rearlock,NULL);
}

void Queue_Enqueue(queue_t *q,int val){
    node_t *tmp = malloc(sizeof(node_t));	//先分配资源，再加锁
    assert(tmp != NULL);
    tmp->val = val;
    tmp->next = NULL;

    pthread_mutex_lock(&q->rearlock);
    q->rear->next = tmp;
    q->rear = tmp;
    pthread_mutex_unlock(&q->rearlock);
}

int Queue_Dequeue(queue_t *q,int *val){
    pthread_mutex_lock(&q->frontlock);
    node_t *dummy = q->front;
    if(dummy->next == NULL){
        pthread_mutex_unlock(&q->frontlock);
        return 0;
    }
    *val = dummy->next->val;   
    q->front = dummy->next;
    pthread_mutex_unlock(&q->frontlock);
    free(dummy);
    return 1;
}

int cnt = 0;
pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;

void* tfun(void* arg){
    queue_t *q = (queue_t*)arg;
    pthread_mutex_lock(&lock);
    Queue_Enqueue(q,++cnt);
    if((cnt & 1) == 0){
        int tmp;
        Queue_Dequeue(q,&tmp);
        printf("%d\n", tmp);
    }
    pthread_mutex_unlock(&lock);
    return NULL;
}

int main(){
    queue_t *q = (queue_t*)malloc(sizeof(queue_t));
    Queue_Init(q);

    pthread_t threads[N];

    for(int i = 0; i < N; ++i)
        pthread_create(&threads[i],NULL,tfun,q);
    
    for(int i = 0; i < N; ++i)
        pthread_join(threads[i],NULL);
    
    node_t *cur = q->front->next;
    while(cur){
        printf("%d->", cur->val);
        cur = cur->next;
    }
    printf("NULL\n");
    return 0;
}
```

仔细研究这段代码，你会发现有两个锁，一个负责队列头，另一个负责队列尾。这两个锁使得入队列操作和出队列操作可以并发执行，因为入队列只访问 rear 锁，而出队列只访问 front 锁。

Michael 和 Scott 使用了一个技巧，添加了一个假节点（在队列初始化的代码里分配的）。 该假节点分开了头和尾操作。研究这段代码，或者输入、运行、测试它，以便更深入地理解它。

队列在多线程程序里广泛使用。然而，这里的队列（只是加了锁）通常不能完全满足这种程序的需求。**更完善的有界队列，在队列空或者满时，能让线程等待**。这是下一章探讨***条件变量（condition variable）***时集中研究的主题。读者需要看仔细了！ 



####	4.并发散列表

我们只关注不需要调整大小（rehash）的简单散列表。支持调整大小还需要一些工作

```c
#define BUCKETS 103

typedef struct hash_t{
    list_t lists[BUCKETS];
}hash_t;

void Hash_Init(hash_t *H){
    for(int i = 0; i < BUCKETS; ++i)
        List_Init(&H->lists[i]);
}

int Hash_Insert(hash_t *H,int key){
    return List_Insert(&H->lists[key%BUCKETS],key);
}

int Hash_Lookup(hash_t *H,int key){
    return List_Lookup(&H->lists[key%BUCKETS],key);
}

```

本例的散列表使用我们之前实现的并发链表（拉链法处理冲突），性能特别好。每个散列桶（每个桶都是一个链表）都有一个锁，而不是整个散列表只有一个锁，从而支持许多并发操作。 图 29.10 展示了并发更新下的散列表的性能。同时，作为比较，我们也展示了单锁链表的性能。可以 看出，这个简单的并发散列表扩展性极好，而链表则相反。

![image-20230419134755606](/images/mdpic/image-20230419134755606.png)

![image-20230419134831725](/images/mdpic/image-20230419134831725.png)



####	5.小结

我们已经介绍了一些并发数据结构，从计数器到链表队列，最后到大量使用的散列表。

同时，我们也学习到：**控制流变化时注意获取锁和释放锁；增加并发不一定能提高性能； 有性能问题的时候再做优化。关于最后一点，避免不成熟的优化（premature optimization）**， 对于所有关心性能的开发者都有用。我们让整个应用的某一小部分变快，却没有提高整体性能，其实没有价值。 当然，我们只触及了高性能数据结构的皮毛。Moir 和 Shavit 的调查提供了更多信息， 包括指向其他来源的链接。特别是，你可能会对其他结构感兴趣（比如 B 树），那么数据库课程会是一个不错的选择。你也可能对根本不用传统锁（互斥锁）的技术感兴趣。这种非阻塞数据结构是有意义的，在常见并发问题的章节中，我们会稍稍涉及。但老实说这是一个广泛领域的知识，远非本书所能覆盖。感兴趣的读者可以自行研究。



##	30 条件变量（同步）

到目前为止，我们已经形成了锁的概念，看到了如何通过硬件和操作系统支持的正确组合来实现锁。然而，锁并不是并发程序设计所需的唯一原语。 

具体来说，在很多情况下，线程需要检查某一条件（condition）满足之后，才会继续运行。例如，父线程需要检查子线程是否执行完毕 [这常被称为 join()]。这种等待如何实现呢？ 我们来看看：

```c
1 void *child(void *arg) {
2 printf("child\n");
3 // XXX how to indicate we are done?
4 return NULL;
5 }
6
7 int main(int argc, char *argv[]) {
8 printf("parent: begin\n");
9 pthread_t c;
10 Pthread_create(&c, NULL, child, NULL); // create child
11 // XXX how to wait for child?
12 printf("parent: end\n");
13 return 0;
14 } 
```

我们期望能看到这样的输出:

```bash
parent: begin
child
parent: end
```

我们可以尝试用一个共享变量，如图 30.2 所示。这种解决方案一般能工作，但是效率低下，因为主线程会自旋检查，浪费 CPU 时间。我们希望有某种方式让父线程休眠，直到等待的条件满足（即子线程完成执行）。

```c
1 volatile int done = 0;
2
3 void *child(void *arg) {
4 printf("child\n");
5 done = 1;
6 return NULL;
7 }
8
9 int main(int argc, char *argv[]) {
10 printf("parent: begin\n"); 
11 pthread_t c;
12 Pthread_create(&c, NULL, child, NULL); // create child
13 while (done == 0)
14 ; // spin
15 printf("parent: end\n");
16 return 0;
17 } 
```

![image-20230419151827382](/images/mdpic/image-20230419151827382.png)

####	1.定义和程序

线程可以使用***条件变量（condition variable）***，来等待一个条件变成真。

条件变量是一个 显式队列，当某些执行状态（即条件，condition）不满足时，线程可以把自己加入队列，等待（waiting）该条件。另外某个线程，当它改变了上述状态时，就可以唤醒一个或者多个 等待线程（通过在该条件上发信号），让它们继续执行。Dijkstra 最早在“私有信号量” 中提出这种思想。Hoare 后来在关于观察者的工作中，将类似的思想称为条件变量。 

要声明这样的条件变量，只要像这样写：pthread_cond_t c;，这里声明 c 是一个条件变量（注意：还需要适当的初始化）。条件变量有两种相关操作：wait()和 signal()。线程要睡眠的时候，调用 wait()。当线程想唤醒等待在某个条件变量上的睡眠线程时，调用 signal()。

```c
pthread_cond_wait(pthread_cond_t *c, pthread_mutex_t *m);
pthread_cond_signal(pthread_cond_t *c);
```

不正确的尝试

```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <pthread.h>
#include <assert.h>

int flag = 0;
pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
pthread_cond_t c = PTHREAD_COND_INITIALIZER;

void* child(void* arg){
    printf("child\n");

    pthread_mutex_lock(&lock);
    flag = 1;
    pthread_cond_signal(&c);
    pthread_mutex_unlock(&lock);
    return NULL;
}

void join(){
    pthread_mutex_lock(&lock);
    while(flag == 0)
        pthread_cond_wait(&c,&lock);
    pthread_mutex_unlock(&lock);
}


int main(){
    printf("parent:begin\n");
    
    pthread_t p;
    pthread_create(&p,NULL,child,NULL);
    join();

    printf("parent:end\n");
    return 0;
}
```

我们常简称为 wait()和 signal()。你可能注意到一点，wait()调用有一个参数，它是互斥量。它假定在 wait()调用时，这个互斥量是已上锁状态。wait()的职责是释放锁，并让调用线程休眠（原子地）。当线程被唤醒时（在另外某个线程发信号给它后），它必须重新获取锁，再返回调用者。

这样复杂的步骤也是为了避免在线程陷入休眠时，产生一些竞态条件。 我们观察一下 join 问题的解决方法，以加深理解。 有两种情况需要考虑：

第一种情况是父线程创建出子线程，但自己继续运行（假设只 有一个处理器），然后马上调用 thr_join()等待子线程。在这种情况下，它会先获取锁，检查 子进程是否完成（还没有完成），然后调用 wait()，让自己休眠。子线程最终得以运行，打印出“child”，并调用 thr_exit()函数唤醒父进程，这段代码会在获得锁后设置状态变量 done， 然后向父线程发信号唤醒它。最后，父线程会运行（从 wait()调用返回并持有锁），释放锁， 打印出“parent:end”。 

第二种情况是，子线程在创建后，立刻运行，设置变量 done 为 1，调用 signal 函数唤醒其他线程（这里没有其他线程），然后结束。父线程运行后，调用 thr_join()时，发现 done 已经是 1 了，就直接返回。 

最后一点说明：你可能看到父线程使用了一个 while 循环，而不是 if 语句来判断是否需要等待。虽然从逻辑上来说没有必要使用循环语句，**但这样做总是好的**（后面我们会加以说明）。 为了确保理解 thr_exit()和 thr_join()中每个部分的重要性，我们来看一些其他的实现。 首先，你可能会怀疑状态变量 done 是否需要。代码像下面这样如何？正确吗？

```c
1 void thr_exit() {
2 	Pthread_mutex_lock(&m);
3 	Pthread_cond_signal(&c);
4	Pthread_mutex_unlock(&m);
5 }
6
7 void thr_join() {
8 	Pthread_mutex_lock(&m);
9 	Pthread_cond_wait(&c, &m); 
10 	Pthread_mutex_unlock(&m);
11 }   	
```

这段代码是有问题的。假设子线程立刻运行，并且调用 thr_exit()。在这种情况下，子线程发送信号，但此时却没有在条件变量上睡眠等待的线程。父线程运行时，就会调用 wait 并卡在那里，没有其他线程会唤醒它。通过这个例子，你应该认识到变量 done 的重要性， 它记录了线程有兴趣知道的值。睡眠、唤醒和锁都离不开它。 下面是另一个糟糕的实现。在这个例子中，我们假设线程在发信号和等待时都不加锁。 会发生什么问题？想想看！

```c
1 void thr_exit() {
2 	done = 1;
3 	Pthread_cond_signal(&c);
4 }
5
6 void thr_join() {
7 	if (done == 0)
8 	Pthread_cond_wait(&c);
9 } 
```

这里的问题是一个**微妙的竞态条件**。具体来说，如果父进程调用 thr_join()，然后检查完 done 的值为 0，然后试图睡眠。但在调用 wait 进入睡眠之前，父进程被中断。子线程修改变量 done 为 1，发出信号，同样没有等待线程。父线程再次运行时，就会长眠不醒，这就惨了。

![image-20230419152343413](/images/mdpic/image-20230419152343413.png)

希望通过这个简单的 join 示例，你可以看到使用条件变量的一些基本要求。为了确保你能理解，我们现在来看一个更复杂的例子：***生产者/消费者（producer/consumer）***或有界缓冲区（bounded-buffer）问题。




####	2.生成者/消费者（有界缓冲区）问题

本章要面对的下一个问题，是生产者/消费者（producer/consumer）问题，也叫作有界缓冲区（bounded buffer）问题。这一问题最早由 Dijkstra 提出。实际上也正是通过研究这一问题，Dijkstra 和他的同事发明了通用的信号量（它可用作锁或条件变量）。 

假设有一个或多个生产者线程和一个或多个消费者线程。生产者把生成的数据项放入缓冲区；消费者从缓冲区取走数据项，以某种方式消费。 很多实际的系统中都会有这种场景。例如，在多线程的网络服务器中，一个生产者将 HTTP 请求放入工作队列（即有界缓冲区），消费线程从队列中取走请求并处理。 

我们在使用管道连接不同程序的输出和输入时，也会使用有界缓冲区，例如 grep foo file.txt | wc -l。这个例子并发执行了两个进程，grep 进程从 file.txt 中查找包括“foo”的行， 写到标准输出；UNIX shell 把输出重定向到管道（通过 pipe 系统调用创建）。管道的另一端 是 wc 进程的标准输入，wc 统计完行数后打印出结果。因此，grep 进程是生产者，wc 是进程是消费者，它们之间是内核中的有界缓冲区，而你在这个例子里只是一个开心的用户。 

**因为有界缓冲区是共享资源，所以我们必须通过同步机制来访问它，以免产生竞态条件**。为了更好地理解这个问题，我们来看一些实际的代码。 首先需要一个共享缓冲区，让生产者放入数据，消费者取出数据。简单起见，我们就 拿一个整数来做缓冲区（你当然可以想到用一个指向数据结构的指针来代替），两个内部函数将值放入缓冲区，从缓冲区取值。

```c
1 int buffer;
2 int count = 0; // initially, empty
3
4 void put(int value) {
5 	assert(count == 0);
6 	count = 1;
7 	buffer = value;
8 }
9
10 int get() {
11 	assert(count == 1);
12 	count = 0;
13 	return buffer;
14 } 
```

很简单，不是吗？put()函数会假设缓冲区是空的，把一个值存在缓冲区，然后把 count 设置为 1 表示缓冲区满了。get()函数刚好相反，把缓冲区清空后（即将 count 设置为 0），并 返回该值。不用担心这个共享缓冲区只能存储一条数据，稍后我们会一般化，用队列保存更多数据项，这会比听起来更有趣。 现在我们需要编写一些函数，知道何时可以访问缓冲区，以便将数据放入缓冲区或从 缓冲区取出数据。条件是显而易见的：仅在 count 为 0 时（即缓冲器为空时），才将数据放 入缓冲器中。仅在计数为 1 时（即缓冲器已满时），才从缓冲器获得数据。如果我们编写同 步代码，让生产者将数据放入已满的缓冲区，或消费者从空的数据获取数据，就做错了（在 这段代码中，断言将触发）。 这项工作将由两种类型的线程完成，其中一类我们称之为生产者（producer）线程，另 一类我们称之为消费者（consumer）线程。图 30.5 展示了一个生产者的代码，它将一个整 数放入共享缓冲区 loops 次，以及一个消费者，它从该共享缓冲区中获取数据（永远不停），每次打印出从共享缓冲区中提取的数据项。

```c
1 void *producer(void *arg) {
2 	int i;
3 	int loops = (int) arg;
4 	for (i = 0; i < loops; i++) {
5 		put(i);
6 	}
7 }
8
9 void *consumer(void *arg) {
10 	int i;
11 	while (1) {
12 		int tmp = get();
13 		printf("%d\n", tmp);
14 	}
15 } 
```

**有问题的方案** 

假设只有一个生产者和一个消费者。显然，put()和 get()函数之中会有临界区，因为 put() 更新缓冲区，get()读取缓冲区。但是，给代码加锁没有用，我们还需别的东西。不奇怪，别的东西就是某些条件变量。在这个（有问题的）首次尝试中（见图 30.6），我们用了条件变量 cond 和相关的锁 mutex。

```c
cond_t cond;
mutex_t mutex;

void *producer(void *arg){
    for (int i = 0; i < loops; i++){
        Pthread_mutex_lock(&mutex);           // p1
        if (count == 1)                       // p2
            Pthread_cond_wait(&cond, &mutex); // p3
        put(i);                               // p4
        Pthread_cond_signal(&cond);           // p5
        Pthread_mutex_unlock(&mutex);         // p6
    }
}

void *consumer(void *arg){
    for (int i = 0; i < loops; i++){
        Pthread_mutex_lock(&mutex);           // c1
        if (count == 0)                       // c2
            Pthread_cond_wait(&cond, &mutex); // c3
        int tmp = get();                      // c4
        Pthread_cond_signal(&cond);           // c5
        Pthread_mutex_unlock(&mutex);         // c6
        printf("%d\n", tmp);
    }
}
```

来看看生产者和消费者之间的信号逻辑。当生产者想要填充缓冲区时，它等待缓冲区变空（p1～p3）。消费者具有完全相同的逻辑，但等待不同的条件——变满（c1～c3）。 当只有一个生产者和一个消费者时，图 30.6 中的代码能够正常运行。但如果有超过一 个线程（例如两个消费者），**这个方案会有两个严重的问题**。哪两个问题？

我们来理解第一个问题，它与等待之前的 if 语句有关。假设有两个消费者（Tc1 和 Tc2）， 一个生产者（Tp）。

首先，一个消费者（Tc1）先开始执行，它获得锁（c1），检查缓冲区是 否可以消费（c2），然后等待（c3）（这会释放锁）。 接着生产者（Tp）运行。它获取锁（p1），检查缓冲区是否满（p2），发现没满就给缓冲区加入一个数字（p4）。然后生产者发出信号，说缓冲区已满（p5）。关键的是，这让第一 个消费者（Tc1）不再睡在条件变量上，进入就绪队列。Tc1 现在可以运行（但还未运行）。 生产者继续执行，直到发现缓冲区满后睡眠（p6,p1-p3）。 这时问题发生了：**另一个消费者（Tc2）抢先执行，消费了缓冲区中的值**（c1,c2,c4,c5,c6， 跳过了 c3 的等待，因为缓冲区是满的）。现在假设 Tc1 运行，在从 wait 返回之前，它获取了锁，然后返回。然后它调用了 get() (p4)，但缓冲区已无法消费！断言触发，代码不能像预 期那样工作。显然，我们应该设法阻止 Tc1 去消费，因为 Tc2 插进来，消费了缓冲区中之前生产的一个值。表 30.1 展示了每个线程的动作，以及它的调度程序状态（就绪、运行、睡眠）随时间的变化。



问题产生的原因很简单：在 Tc1 被生产者唤醒后，但在它运行之前，缓冲区的状态改变 了（由于 Tc2）。**发信号给线程只是唤醒它们，暗示状态发生了变化**（在这个例子中，就是值 已被放入缓冲区），**但并不会保证在它运行之前状态一直是期望的情况**。信号的这种释义常称为 ***Mesa 语义（Mesa semantic）***，为了纪念以这种方式建立条件变量的首次研究。 另一种释义是 Hoare 语义（Hoare semantic），虽然实现难度大，但是会保证被唤醒线程立刻执行。实际上，几乎所有系统都采用了 Mesa 语义。



**较好但仍有问题的方案：使用 While 语句替代 If** 

幸运的是，修复这个问题很简单（见图 30.7）：把 if 语句改为 while。当消费者 Tc1 被唤 醒后，立刻再次检查共享变量（c2）。如果缓冲区此时为空，消费者就会回去继续睡眠（c3）。 生产者中相应的 if 也改为 while（p2）。

```c
cond_t cond;
mutex_t mutex;

void *producer(void *arg){
    int i;
    for (i = 0; i < loops; i++){
        Pthread_mutex_lock(&mutex);           // p1
        while (count == 1)                    // p2
            Pthread_cond_wait(&cond, &mutex); // p3
        put(i);                               // p4
        Pthread_cond_signal(&cond);           // p5
        Pthread_mutex_unlock(&mutex);         // p6
    }
}

void *consumer(void *arg){
    int i;
    for (i = 0; i < loops; i++){
        Pthread_mutex_lock(&mutex);           // c1
        while (count == 0)                    // c2
            Pthread_cond_wait(&cond, &mutex); // c3
        int tmp = get();                      // c4
        Pthread_cond_signal(&cond);           // c5
        Pthread_mutex_unlock(&mutex);         // c6
        printf("%d\n", tmp);
    }
}
```

由于 Mesa 语义，我们要记住一条关于条件变量的简单规则：**总是使用 *while 循环（always use while loop）***。虽然有时候不需要重新检查条件，但这样做总是安全的，做了就开心了。 

但是，这段代码仍然有一个问题，也是上文提到的两个问题之一。你能想到吗？它和 我们只用了一个条件变量有关。尝试弄清楚这个问题是什么，再继续阅读。想一下！

我们来确认一下你想得对不对。假设两个消费者（Tc1 和 Tc2）先运行，都睡眠了（c3）。 生产者开始运行，在缓冲区放入一个值，唤醒了一个消费者（假定是 Tc1），并开始睡眠。现在是一个消费者马上要运行（Tc1），两个线程（Tc2 和 Tp）都等待在同一个条件变量上。问题马上就要出现了：消费者 Tc1 醒过来并从 wait()调用返回（c3），重新检查条件（c2），发现缓冲区是满的， 消费了这个值（c4）。这个消费者然后在该条件上发信号（c5），唤醒一个在睡眠的线程。但是，应该唤醒哪个线程呢？ 

因为消费者已经清空了缓冲区，**很显然，应该唤醒生产者。但是，如果它唤醒了 Tc2**（这绝对是可能的，取决于等待队列是如何管理的），问题就出现了。具体来说，**消费者 Tc2 会醒过来，发现队列为空（c2），又继续回去睡眠（c3）**。生产者 Tp 刚才在缓冲区中放了一个值，现在在睡眠。另一个消费者线程 Tc1 也回去睡眠了。**3 个线程都在睡眠**，显然是一个缺陷。

**信号显然需要，但必须更有指向性。消费者不应该唤醒消费者，而应该只唤醒生产者，反之亦然。**



**单值缓冲区的生产者/消费者方案** 

解决方案也很简单：使用两个条件变量，而不是一个，以便正确地发出信号，在系统状态改变时，哪类线程应该唤醒。下面展示了最终的代码。

```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <pthread.h>
#include <assert.h>

pthread_cond_t empty = PTHREAD_COND_INITIALIZER, fill = PTHREAD_COND_INITIALIZER;
pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;

#define loops 10
int count = 0;

void* producer(void* arg){
    for(int i = 0; i < loops; ++i){
        pthread_mutex_lock(&lock);
        while(count == 1)							// 保证是理想的状态
            pthread_cond_wait(&empty,&lock);
        ++count;
        pthread_mutex_signal(&fill);				// 指向式睡眠
        pthread_mutex_unlock(&lock);
    }
}

void* consumer(void* arg){
    for(int i = 0; i < loops; ++i){
        pthread_mutex_lock(&lock);
        while(count == 0)
            pthread_cond_wait(&fill,&lock);
        int tmp = count;
        --count;
        pthread_mutex_signal(&empty);
        pthread_mutex_unlock(&lock);
        printf("%d\n", tmp);
    }

}
```



**最终的生产者/消费者方案** 

我们现在有了可用的生产者/消费者方案，但不太通用。我们最后的修改是提高并发和效率。具体来说，增加更多缓冲区槽位，这样在睡眠之前，可以生产多个值。同样，睡眠 之前可以消费多个值。单个生产者和消费者时，这种方案因为**上下文切换少**，提高了效率。 多个生产者和消费者时，它甚至支持并发生产和消费，从而提高了并发。幸运的是，和现有方案相比，改动也很小。 

第一处修改是缓冲区结构本身，以及对应的 put()和 get()方法。我们还稍稍修改了生产者和消费者的检查条件，以便决定是否要睡眠。图 30.10 展示了最终的等待和信 号逻辑。生产者只有在缓冲区满了的时候才会睡眠（p2），消费者也只有在队列为空的时候睡眠（c2）。至此，我们解决了生产者/消费者问题。

```c
int buffer[MAX];
int fill = 0;
int use = 0;
int count = 0;

void put(int value){
    buffer[fill] = value;
    fill = (fill + 1) % MAX;
    count++;
}

int get(){
    int tmp = buffer[use];
    use = (use + 1) % MAX;
    count--;
    return tmp;
}
cond_t empty, fill;
mutex_t mutex;

void *producer(void *arg){
    int i;
    for (i = 0; i < loops; i++){
        Pthread_mutex_lock(&mutex);            // p1
        while (count == MAX)                   // p2	缓冲区满了，生产者才睡眠
            Pthread_cond_wait(&empty, &mutex); // p3
        put(i);                                // p4
        Pthread_cond_signal(&fill);            // p5
        Pthread_mutex_unlock(&mutex);          // p6
    }
}

void *consumer(void *arg){
    int i;
    for (i = 0; i < loops; i++){
        Pthread_mutex_lock(&mutex);           // c1
        while (count == 0)                    // c2		缓冲区空了，消费者才睡眠
            Pthread_cond_wait(&fill, &mutex); // c3
        int tmp = get();                      // c4
        Pthread_cond_signal(&empty);          // c5
        Pthread_mutex_unlock(&mutex);         // c6
        printf("%d\n", tmp);
    }
}
```

![image-20230419161302591](/images/mdpic/image-20230419161302591.png)

####	3.覆盖条件

现在再来看条件变量的一个例子。这段代码摘自 Lampson 和 Redell 关于飞行员的论文，同一个小组首次提出了上述的 ***Mesa 语义（Mesa semantic***，他们使用的语言是 Mesa， 因此而得名）。 他们遇到的问题通过一个简单的例子就能说明，在这个例子中，是一个简单的多线程内存分配库：

```c
// how many bytes of the heap are free?
int bytesLeft = MAX_HEAP_SIZE;

// need lock and condition too
cond_t c;
mutex_t m;

void *
allocate(int size){
    Pthread_mutex_lock(&m);
    while (bytesLeft < size)
        Pthread_cond_wait(&c, &m);
    void *ptr = ...; // get mem from heap
    bytesLeft -= size;
    Pthread_mutex_unlock(&m);
    return ptr;
}

void free(void *ptr, int size){
    Pthread_mutex_lock(&m);
    bytesLeft += size;
    Pthread_cond_signal(&c); // whom to signal??
    Pthread_mutex_unlock(&m);
}
```

从代码中可以看出，当线程调用进入内存分配代码时，它可能会因为内存不足而等待。 相应的，线程释放内存时，会发信号说有更多内存空闲。但是，代码中有一个问题：**应该唤醒哪个等待线程（可能有多个线程）？** 

考虑以下场景。假设目前没有空闲内存，线程 Ta 调用 allocate(100)，接着线程 Tb 请求 较少的内存，调用 allocate(10)。Ta和 Tb 都等待在条件上并睡眠，没有足够的空闲内存来满 足它们的请求。 这时，假定第三个线程 Tc调用了 free(50)。遗憾的是，当它发信号唤醒等待线程时，可能不会唤醒申请 10 字节的 Tb 线程。而 Ta 线程由于内存不够，仍然等待。因为不知道唤醒 哪个（或哪些）线程，所以图中代码无法正常工作。 Lampson 和 Redell 的解决方案也很直接：用 **pthread_cond_broadcast()**代替上述代码中的 pthread_cond_signal()，唤醒所有的等待线程。这样做，确保了所有应该唤醒的线程都被唤醒。当然，不利的一面是可能会影响性能，因为不必要地唤醒了其他许多等待的线程，它们本来（还）不应该被唤醒。这些线程被唤醒后，重新检查条件，马上再次睡眠。 Lampson 和 Redell 把这种条件变量叫作***覆盖条件（covering condition）***，因为它能覆盖所有需要唤醒线程的场景（保守策略）。成本如上所述，就是太多线程被唤醒。聪明的读者可能发现，**在单个条件变量的生产者/消费者问题中，也可以使用这种方法**。但是，在这个例子中，我们有更好的方法，因此用了它。一般来说，如果你发现程序只有改成广播信号时才能工作（但你认为不需要），可能是程序有缺陷，修复它！但在上述内存分配的例子中， 广播可能是最直接有效的方案。



**jyy的producer/consumer**

```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <pthread.h>
#include <assert.h>

int n, count = 0;

pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
pthread_cond_t cv = PTHREAD_COND_INITIALIZER;

#define CAN_PRODUCE (count < n)
#define CAN_CONSUME (count > 0)

void* Tproduce(void*){
    while (1){
        pthread_mutex_lock(&lock);
        while (!CAN_PRODUCE){
            pthread_cond_wait(&cv, &lock);
        }
        printf("(");    count++;
        pthread_cond_broadcast(&cv);				// 覆盖条件
        pthread_mutex_unlock(&lock);
    }
}

void* Tconsume(void*){
    while (1){
        pthread_mutex_lock(&lock);
        while (!CAN_CONSUME){
            pthread_cond_wait(&cv, &lock);
        }
        printf(")");    count--;
        pthread_cond_broadcast(&cv);
        pthread_mutex_unlock(&lock);
    }
}

int main(int argc, char *argv[]){
    assert(argc == 3);
    n = atoi(argv[1]);
    int T = atoi(argv[2]);
    setbuf(stdout, NULL);

    pthread_t Tp[T], Tc[T];
    

    for (int i = 0; i < T; i++){
        pthread_create(&Tp[i],NULL,Tproduce,NULL);
        pthread_create(&Tc[i],NULL,Tconsume,NULL);
    }

    for(int i = 0; i < T; i++){
        pthread_join(Tp[i],NULL);
        pthread_join(Tc[i],NULL);
    }

}
```



####	小结

我们看到了引入锁之外的另一个重要同步原语：**条件变量**。当某些程序状态不符合要求时，通过允许线程进入休眠状态，**条件变量使我们能够漂亮地解决许多重要的同步问题， 包括著名的（仍然重要的）生产者/消费者问题，以及覆盖条件。**



##	31 信号量

我们现在知道，需要锁和条件变量来解决各种相关的、有趣的并发问题。多年前，首先认识到这一点的人之中，有一个就是 Edsger Dijkstra。 他引入了名为信号量的同步原语，正是这里我们要学习的。事实上，Dijkstra 及其同事发明了信号量，作为与同步有关的所有工作的唯一原语。你会看到，可以使用信号量作为锁和条件变量。

![image-20230419195224687](/images/mdpic/image-20230419195224687.png)

####	1.信号量的定义

信号量是有一个整数值的对象，可以用两个函数来操作它。在 POSIX 标准中，是 sem_wait()和 sem_post()。因为信号量的初始值能够决定其行为，所以首先要初始化信号量，才能调用其他函数与之交互：

```c
#include <semaphore.h>
sem_t s;
sem_init(&s, 0, 1); 
```

其中申明了一个信号量 s，通过第三个参数，将它的值初始化为 1。sem_init()的第二个参数，在我们看到的所有例子中都设置为 0，表示信号量是在同一进程的多个线程共享的。 读者可以参考手册，了解信号量的其他用法（即如何用于跨不同进程的同步访问），这要求第二个参数用不同的值。

信号量初始化之后，我们可以调用 sem_wait()或 sem_post()与之交互。

下面展示了这两个函数的不同行为。 我们暂时不关注这两个函数的实现，这显然是需要注意的。多个线程会调用 sem_wait() 和 sem_post()，显然需要管理这些临界区。我们首先关注如何使用这些原语，稍后再讨论如何实现。

```c
int sem_wait(sem_t *s){
	将信号量 s 的值减一
	如果信号量 s 的值为负则进入睡眠（阻塞）
}

int sem_post(sem_t *s){
    将信号量 s 的值增加 1
	如果有一个或多个线程在等待，唤醒一个
}


P - prolaag (try + decrease/down/wait/acquire)

试着从袋子里取一个球
如果拿到了，离开
如果袋子空了，排队等待
    
V - verhoog (increase/up/post/signal/release)

往袋子里放一个球
如果有人在等球，他就可以拿走刚放进去的球了
放球-拿球的过程实现了同步
```

我们应该讨论这些接口的几个突出方面。首先，sem_wait()要么立刻返回（调用 sem_wait() 时，信号量的值大于等于 1），要么会让调用线程挂起，直到之后的一个 post 操作。当然， 也可能多个调用线程都调用 sem_wait()，因此都在队列中等待被唤醒。 其次，sem_post()并没有等待某些条件满足。它直接增加信号量的值，如果有等待线程， 唤醒其中一个。 最后，当信号量的值为负数时，这个值就是等待线程的个数。虽然这个值通常不会暴露给信号量的使用者，但这个恒定的关系值得了解，可能有助于记住信号量的工作原理。 **先（暂时）不用考虑信号量内的竞争条件，假设这些操作都是原子的**。我们很快就会用锁和条件变量来实现。



####	2.二值信号量（锁）

现在我们要使用信号量了。信号量的第一种用法是我们已经熟悉的：用信号量作为锁。 在图 31.3 所示的代码片段里，我们直接把临界区用一对 sem_wait()/sem_post()环绕。但是，为了使这段代码正常工作，信号量 m 的初始值（图中初始化为 X）是至关重要的。X 应该是多少呢？

```c
sem_t m;
sem_init(&m, 0, X); // initialize semaphore to X; what should X be?

sem_wait(&m);
// critical section here
sem_post(&m);
```

回顾 sem_wait()和 sem_post()函数的定义，我们发现初值应该是 1。 为了说明清楚，我们假设有两个线程的场景。第一个线程（线程 0）调用了 sem_wait()， 它把信号量的值减为 0。然后，它只会在值小于 0 时等待。因为值是 0，调用线程从函数返回并继续，线程 0 现在可以自由进入临界区。线程 0 在临界区中，如果没有其他线程尝试 获取锁，当它调用 sem_post()时，会将信号量重置为 1（因为没有等待线程，不会唤醒其他线程）。表 31.1 追踪了这一场景。

![image-20230419200658653](/images/mdpic/image-20230419200658653.png)

如果线程 0 持有锁（即调用了 sem_wait()之后，调用 sem_post()之前），另一个线程（线 程 1）调用 sem_wait()尝试进入临界区，那么更有趣的情况就发生了。这种情况下，线程 1 把信号量减为−1，然后等待**（自己睡眠，放弃处理器）**。线程 0 再次运行，它最终调用 sem_post()，将信号量的值增加到 0，唤醒等待的线程（线程 1），然后线程 1 就可以获取锁。 线程 1 执行结束时，再次增加信号量的值，将它恢复为 1。 表 31.2 追踪了这个例子。除了线程的动作，表中还显示了每一个线程的调度程序状态 （scheduler state）：运行、就绪（即可运行但没有运行）和睡眠。特别要注意，当线程 1 尝试获 取已经被持有的锁时，陷入睡眠。只有线程 0 再次运行之后，线程 1 才可能会唤醒并继续运行。



![image-20230419200648385](/images/mdpic/image-20230419200648385.png)

如果你想追踪自己的例子，那么请尝试一个场景，多个线程排队等待锁。在这样的追踪中，信号量的值会是什么？ 我们可以用信号量来实现锁了。因为锁只有两个状态（持有和没持有），所以这种用法 有时也叫作***二值信号量（binary semaphore）***。事实上这种信号量也有一些更简单的实现，我们这里使用了更为通用的信号量作为锁。



####	3.信号量用作条件变量

信号量也可以用在一个线程暂停执行，等待某一条件成立的场景。例如，一个线程要等待一个链表非空，然后才能删除一个元素。在这种场景下，通常一个线程等待条件成立， 另外一个线程修改条件并发信号给等待线程，从而唤醒等待线程。因为等待线程在等待某些条件（condition）发生变化，所以我们将信号量作为***条件变量（condition variable）***。 下面是一个简单例子。假设一个线程创建另外一线程，并且等待它结束。

```c
sem_t s;

void *child(void *arg){
    printf("child\n");
    sem_post(&s); // signal here: child is done
    return NULL;
}

int main(int argc, char *argv[]){
    sem_init(&s, 0, X); // what should X be?
    printf("parent: begin\n");
    pthread_t c;
    Pthread_create(c, NULL, child, NULL);
    sem_wait(&s); // wait here for child
    printf("parent: end\n");
    return 0;
}
```

该程序运行时，我们希望能看到这样的输出： 

parent: begin 

child 

parent: end 

然后问题就是如何用信号量来实现这种效果。结果表明，答案也很容易理解。从代码中可知，父线程调用 sem_wait()，子线程调用 sem_post()，父线程等待子线程执行完成。但是，问题来了：信号量的初始值应该是多少？ （再想一下，然后继续阅读）

当然，答案是信号量初始值应该是 0。有两种情况需要考虑。第一种，父线程创建了子线程，但是子线程并没有运行。这种情况下，父线程调用 sem_wait()会先于子 线程调用 sem_post()。我们希望父线程等待子线程运行。为此，唯一的办法是让信号量的值 不大于 0。因此，0 为初值。父线程运行，将信号量减为−1，然后睡眠等待；子线程运行的 时候，调用 sem_post()，信号量增加为 0，唤醒父线程，父线程然后从 sem_wait()返回，完成该程序。

第二种情况是子线程在父线程调用 sem_wait()之前就运行结束。在这种情况下， 子线程会先调用 sem_post()，将信号量从 0 增加到 1。然后当父线程有机会运行时，会调用 sem_wait()，发现信号量的值为 1。于是父线程将信号量从 1 减为 0，没有等待，直接从 sem_wait()返回，也达到了预期效果。



####	4.生成者/消费者问题



**第一次尝试** 





**解决方案：增加互斥**





**避免死锁**

假设有两个线程，一个生产者和一个消费者。

消费者首先运行，获得锁，然后对 full 信号量执行 sem_wait() 。因为还没有数据，所以消费者阻塞，让出 CPU。但是，重要的是，**此时消费者仍然持有锁**。 

然后生产者运行。假如生产者能够运行，它就能生产数据并唤醒消费者线程。遗憾的是， 它首先对二值互斥信号量调用 sem_wait()（p0 行）。锁已经被持有，因此生产者也被卡住。 

这里出现了一个循环等待。**消费者持有互斥量，等待在 full 信号量上。生产者可以发送 full 信号，却在等待互斥量**。因此，生产者和消费者互相等待对方——典型的死锁。



**最后，可行的方案** 

要解决这个问题，只需减少锁的作用域。可以看到，我们把获取和释放互斥量的操作调整为紧挨着临界区，把 full、empty 的唤醒和等待操作调整到锁 外面。结果得到了简单而有效的有界缓冲区，多线程程序的常用模式。现在理解，将来使用。 未来的岁月中，你会感谢我们的。至少在期末考试遇到这个问题时，你会感谢我们。

```c
sem_t empty;
sem_t full;
sem_t mutex;

void *producer(void *arg){
    for (int i = 0; i < loops; i++)
    {
        sem_wait(&empty); // line p1
        sem_wait(&mutex); // line p1.5 (MOVED MUTEX HERE...)
        put(i);           // line p2
        sem_post(&mutex); // line p2.5 (... AND HERE)
        sem_post(&full);  // line p3
    }
}

void *consumer(void *arg){
    for (int i = 0; i < loops; i++){
        sem_wait(&full);  // line c1
        sem_wait(&mutex); // line c1.5 (MOVED MUTEX HERE...)
        int tmp = get();  // line c2
        sem_post(&mutex); // line c2.5 (... AND HERE)
        sem_post(&empty); // line c3
        printf("%d\n", tmp);
    }
}

int main(int argc, char *argv[]){
    // ...
    sem_init(&empty, 0, MAX); // MAX buffers are empty to begin with...
    sem_init(&full, 0, 0);    // ... and 0 are full
    sem_init(&mutex, 0, 1);   // mutex=1 because it is a lock
    // ...
}
```





####	5.读写锁

另一个经典问题源于对更加灵活的锁定原语的渴望，它承认不同的数据结构访问可能需要不同类型的锁。

例如，一个并发链表有很多插入和查找操作。插入操作会修改链表的状态（因此传统的临界区有用），而查找操作只是读取该结构，只要没有进行插入操作，我们可以并发的执行多个查找操作。

**读者—写者锁（reader-writer lock）**就是用来完成这种操作的。

```c
typedef struct _rwlock_t{
    sem_t lock;      // binary semaphore (basic lock)
    sem_t writelock; // used to allow ONE writer or MANY readers
    int readers;     // count of readers reading in critical section
} rwlock_t;

void rwlock_init(rwlock_t *rw){
    rw->readers = 0;
    sem_init(&rw->lock, 0, 1);
    sem_init(&rw->writelock, 0, 1);
}

void rwlock_acquire_readlock(rwlock_t *rw){
    sem_wait(&rw->lock);
    rw->readers++;
    if (rw->readers == 1)
        sem_wait(&rw->writelock); // first reader acquires writelock 
    							  // 第一个读者获得写锁
    sem_post(&rw->lock);
}

void rwlock_release_readlock(rwlock_t *rw){
    sem_wait(&rw->lock);
    rw->readers--;
    if (rw->readers == 0)
        sem_post(&rw->writelock); // last reader releases writelock
    							  // 最后一个读者释放写锁
    sem_post(&rw->lock);
}

void rwlock_acquire_writelock(rwlock_t *rw){
    sem_wait(&rw->writelock);
}

void rwlock_release_writelock(rwlock_t *rw){
    sem_post(&rw->writelock);
}
```

读锁的获取和释放操作更加吸引人。获取读锁时，读者首先要获取 lock，然后增加 reader 变量，追踪目前有多少个读者在访问该数据结构。重要的步骤然后在 rwlock_acquire_readlock() 内发生，当第一个读者获取该锁时。在这种情况下，读者也会获取写锁，即在 writelock 信号量上调用 sem_wait()，最后调用 sem_post()释放 lock。 

一旦一个读者获得了读锁，其他的读者也可以获取这个读锁。但是，想要获取写锁的线程，就必须等到所有的读者都结束。最后一个退出的写者在“writelock”信号量上调用sem_post()，从而让等待的写者能够获取该锁。



这一方案可行（符合预期），但有一些缺陷，**尤其是公平性。读者很容易饿死写者**。存在复杂一些的解决方案，也许你可以想到更好的实现？提示：有写者等待时，如何能够避免更多的读者进入并持有锁。 最后，应该指出，读者-写者锁还有一些注意点。它们通常加入了更多开锁（尤其是更复杂的实现），因此和其他一些简单快速的锁相比，读者写者锁在性能方面没有优势。 无论哪种方式，它们都再次展示了如何以有趣、有用的方式来使用信号量。



####	6.哲学家就餐问题

哲学家就餐问题（dining philosopher’s problem）是一个著名的并发问题，它由 Dijkstra 提出来并解决。这个问题之所以出名， 是因为它很有趣，引人入胜，但其实用性却不强。 可是，它的名气让我们在这里必须讲。实际上， 你可能会在面试中遇到这一问题，假如老师没有提过，导致你们没有通过面试，你们会责怪操作系统老师的。

假定有 5 位“哲学家”围着一个圆桌。每两位哲学家之间有一把餐叉（一共 5 把）。哲学家有时要思考一会，不需要餐叉；有时又要就餐。而一位哲学家只有同时拿到了左手边和右手边的两把餐叉，才能吃到东西。关于餐叉的竞争以及随之而来的同步问题，就是我们在并发编程中研究它的原因。 

![image-20230419203210325](/images/mdpic/image-20230419203210325.png)

下面是每个哲学家的基本循环：

```c
while(1){
    think();
    getforks();
    eat();
    putforks();
}
```

关键的挑战就是如何实现 getforks()和 putforks()函数，保证没有死锁，没有哲学家饿死， 并且并发度更高（尽可能让更多哲学家同时吃东西）。 

根据 Downey 的解决方案，我们会用一些辅助函数，帮助构建解决方案。它们是：

```c
int left(int p) { return p; }
int right(int p) { return (p + 1) % 5; } 
```

如果哲学家 p 希望用左手边的叉子，他们就调用 left(p)。类似地，右手边的叉子就用 right(p)。模运算解决了最后一个哲学家（p = 4）右手边叉子的编号问题，就是餐叉 0。 

我们需要一些信号量来解决这个问题。假设需要 5 个，每个餐叉一个：sem_t forks[5]。



有问题的解决方案 我们开始第一次尝试。假设我们把每个信号量（在 fork 数组中）都用 1 初始化。同时 假设每个哲学家知道自己的编号（p）。我们可以写出 getforks()和 putforks()函数:

```c
void getforks(){
    sem_wait(forks[left(p)]);
    sem_wait(forks[right(p)]);
}

void putforks(){
    sem_post(forks[left(p)]);
    sem_post(forks[right(p)]);
}
```

这个（有问题的）解决方案背后的思路如下。为了拿到餐叉，我们依次获取每把餐叉的锁——先是左手边的，然后是右手边的。结束就餐时，释放掉锁。很简单，不是吗？但是， 在这个例子中，简单是有问题的。你能看到问题吗？想一想。 问题是***死锁（deadlock）***。

假设每个哲学家都拿到了左手边的餐叉，他们每个都会阻塞住，并且一直等待另一个餐叉。具体来说，哲学家 0 拿到了餐叉 0，哲学家 1 拿到了餐叉 1， 哲学家 2 拿到餐叉 2，哲学家 3 拿到餐叉 3，哲学家 4 拿到餐叉 4。所有的餐叉都被占有了， 所有的哲学家都阻塞着，并且等待另一个哲学家占有的餐叉。我们在后续章节会深入学习死锁，这里只要知道这个方案行不通就可以了。

**一种方案：破除依赖**

解决上述问题最简单的方法，就是修改某个或者某些哲学家的取餐叉顺序。事实上， Dijkstra 自己也是这样解决的。具体来说，假定哲学家 4（编写最大的一个）取餐叉的顺序不同：

```c
void getforks(){
    if (p == 4){
        sem_wait(forks[right(p)]);
        sem_wait(forks[left(p)]);
    }
    else{
        sem_wait(forks[left(p)]);
        sem_wait(forks[right(p)]);
    }
} 
```

因为最后一个哲学家会尝试先拿右手边的餐叉，然后拿左手边，所以不会出现每个哲学家都拿着一个餐叉，卡住等待另一个的情况，等待循环被打破了。想想这个方案的后果， 让你自己相信它有效。 

还有其他一些类似的“著名”问题，比如***吸烟者问题（cigarette smoker’s problem）***，***理发师问题（sleeping barber problem）***。大多数问题只是让我们去理解并发，某些问题的名字 很吸引人。感兴趣的读者可以去查阅相关资料，或者通过一些更实际的思考去理解并发行为。



jyy解决哲学家问题

```c
// 利用信号量限制上座人数
#include "thread.h"
#include "thread-sync.h"

#define N 5

sem_t table, avail[N];

void Tphilosopher(int id) {
  int lhs = (id + N - 1) % N;
  int rhs = id % N;
  while (1) {
    // Come to table
    P(&table);

    P(&avail[lhs]);
    printf("+ %d by T%d\n", lhs, id);
    P(&avail[rhs]);
    printf("+ %d by T%d\n", rhs, id);

    // Eat

    printf("- %d by T%d\n", lhs, id);
    printf("- %d by T%d\n", rhs, id);
    V(&avail[lhs]);
    V(&avail[rhs]);

    // Leave table
    V(&table);
  }
}

int main() {
  SEM_INIT(&table, N - 1);
  for (int i = 0; i < N; i++) {
    SEM_INIT(&avail[i], 1);
  }
  for (int i = 0; i < N; i++) {
    create(Tphilosopher);
  }
}
```









####	7.如何实现信号量

最后，我们用底层的同步原语（锁和条件变量），来实现自己的信号量，名字叫作 Zemaphore。这个任务相当简单：

```c
typedef struct _Zem_t{
    int value;
    pthread_cond_t cond;
    pthread_mutex_t lock;
} Zem_t;

// only one thread can call this
void Zem_init(Zem_t *s, int value){
    s->value = value;
    Cond_init(&s->cond);
    Mutex_init(&s->lock);
}

void Zem_wait(Zem_t *s){
    Mutex_lock(&s->lock);
    while (s->value <= 0)
        Cond_wait(&s->cond, &s->lock);
    s->value--;
    Mutex_unlock(&s->lock);
}

void Zem_post(Zem_t *s){
    Mutex_lock(&s->lock);
    s->value++;
    Cond_signal(&s->cond);
    Mutex_unlock(&s->lock);
}
```

可以看到，我们只用了一把锁、一个条件变量和一个状态的变量来记录信号量的值。 请自己研究这些代码，直到真正理解它。去做吧！ 我们实现的 Zemaphore 和 Dijkstra 定义的信号量有一点细微区别，就是我们没有保持当信号量的值为负数时，让它反映出等待的线程数。事实上，该值永远不会小于 0。这一行为更容易实现，并符合现有的 Linux 实现。

很奇怪，**利用信号量来实现锁和条件变量，是棘手得多的问题**。某些富有经验的并发程序员曾经在 Windows 环境下尝试过，随之而来的是很多缺陷。你自己试一下，看看 是否能明白为什么使用信号量实现条件变量比看起来更困难。



####	8.小结

信号量是编写并发程序的强大而灵活的原语。有程序员会因为简单实用，只用信号量， 不用锁和条件变量。

条件变量可以说是信号量的特例。但是，信号量往往更加困难，尽量用条件变量来求解问题。

本章展示了几个经典问题和解决方案。如果你有兴趣了解更多，有许多资料可以参考。 Allen Downey 关于并发和使用信号量编程的书就很好（免费的参考资料）。该书包括 了许多谜题，你可以研究它们，从而深入理解具体的信号量和一般的并发。成为一个并发 专家需要多年的努力，学习本课程之外的内容，无疑是掌握这个领域的关键。





##	32 常见的并发问题



####	1.有哪些类型的缺陷

在复杂的并发程序中，有哪些类型的缺陷呢？

根据最新研究表示，大多数是非死锁类型。小部分死锁类型。



####	2.非死锁缺陷

非死锁性问题主要有两种：原子性违反（atomicity violation, AV）和顺序违反（order violation）。



**原子性违反**

```c
Thread 1::
if (thd->proc_info) {
...
fputs(thd->proc_info, ...);
...
}

Thread 2::
thd->proc_info = NULL;
```

这个例子我们可以很明显的看到，如果Thread 1检查完proc_info后，在fputs调用前被中断，Thread运行把指针置为空；当第一个线程恢复运行后，将会对空指针引用，导致崩溃。

根据 Lu 等人，更正式的违反原子性的定义是：**“违反了多次内存访问中预期的可串行性（即代码段本意是原子的，但在执行中并没有强制实现原子性）”**。在我们的例子中， proc_info 的非空检查和 fputs()调用打印 proc_info 是假设原子的，当假设不成立时，代码就出问题了。

这种问题的修复通常（但不总是）很简单。加锁即可。

```c
pthread_mutex_t proc_info_lock = PTHREAD_MUTEX_INITIALIZER;

Thread 1::
pthread_mutex_lock(&proc_info_lock);
if (thd->proc_info) {
...
fputs(thd->proc_info, ...);
...
}
pthread_mutex_unlock(&proc_info_lock); 

Thread 2::
pthread_mutex_lock(&proc_info_lock);
thd->proc_info = NULL;
pthread_mutex_unlock(&proc_info_lock); 
```



**顺序违反**

```c
Thread 1::
void init() {
...
mThread = PR_CreateThread(mMain, ...);
...
}

Thread 2::
void mMain(...) {
...
mState = mThread->State;
...
} 
```

你可能已经发现，线程 2 的代码中似乎假定变量 mThread 已经被初始化了（不为空）。 然而，如果线程 1 并没有首先执行，线程 2 就可能因为引用空指针奔溃（假设 mThread 初始值为空；否则，可能会产生更加奇怪的问题，因为线程 2 中会读到任意的内存位置	并引用）。 **违反顺序更正式的定义是：“两个内存访问的预期顺序被打破了（即 A 应该在 B 之前执 行，但是实际运行中却不是这个顺序）**。 我们通过强制顺序来修复这种缺陷。正如之前详细讨论的，条件变量（condition variables）就是一种简单可靠的方式，在现代代码集中加入这种同步。在上面的例子中，我们可以把代码修改成这样：

```c
pthread_mutex_t mtLock = PTHREAD_MUTEX_INITIALIZER;
pthread_cond_t mtCond = PTHREAD_COND_INITIALIZER;
int mtInit = 0;

Thread 1::
void init() {
...
mThread = PR_CreateThread(mMain, ...);

// signal that the thread has been created...
pthread_mutex_lock(&mtLock);
mtInit = 1;
pthread_cond_signal(&mtCond); 
pthread_mutex_unlock(&mtLock);
...
}

Thread 2::
void mMain(...) {
...
// wait for the thread to be initialized...
pthread_mutex_lock(&mtLock);
while (mtInit == 0)
	pthread_cond_wait(&mtCond, &mtLock);
pthread_mutex_unlock(&mtLock);

mState = mThread->State;
...
} 
```

为了保证顺序，我们可以使用条件变量和信号量。

**非死锁缺陷：小结**

Lu 等人的研究中，大部分（97%）的非死锁问题是违反原子性和违反顺序这两种。因此，程序员仔细研究这些错误模式，应该能够更好地避免它们。此外，随着更自动化的代码检查工具的发展，它们也应该关注这两种错误，因为开发中发现的非死锁问题大部分都 是这两种。

然而，并不是所有的缺陷都像我们举的例子一样，这么容易修复。有些问题需要对应用程序的更深的了解，以及大量代码及数据结构的调整。阅读 Lu 等人的优秀（可读性强） 的论文，了解更多细节。



####	3.死锁缺陷

除了上面提到的并发缺陷，死锁（deadlock）是一种在许多复杂并发系统中出现的经典问题。

例如，当线程 1 持有锁 L1，正在等待另外一个锁 L2，而线程 2 持有锁 L2，却在等 待锁 L1 释放时，死锁就产生了。以下的代码片段就可能出现这种死锁：

```c
Thread 1: Thread 2:
lock(L1); lock(L2);
lock(L2); lock(L1); 
```

这段代码运行时，不是一定会出现死锁的。当线程 1 占有锁 L1，上下文切换到线程 2。 线程 2 锁住 L2，试图锁住 L1。这时才产生了死锁，两个线程互相等待。如下图所示，其中的**圈（cycle）**表明了死锁。



![image-20230423184122118](/images/mdpic/image-20230423184122118.png)

**为什么会发生死锁？**

其中一个原因是在大型的代码库里，组件之间会有复杂的依赖。以操作系统为例。虚拟内存系统在需要访问文件系统才能从磁盘读到内存页；文件系统随后又要和虚拟内存交互，去申请一页内存，以便存放读到的块。因此，在设计大型系统的锁机制时，你必须要仔细地去避免循环依赖导致的死锁。 

另一个原因是封装（encapsulation）。软件开发者一直倾向于隐藏实现细节，以模块化的方式让软件开发更容易。然而，模块化和锁不是很契合。某些看起来没有关系的接口可能会导致死锁。



**产生死锁的条件** 

死锁的产生需要如下 4 个条件：

- 互斥：线程对于需要的资源进行互斥的访问（例如一个线程抢到锁）。
- 持有并等待：线程持有了资源（例如已将持有的锁），同时又在等待其他资源（例如，需要获得的锁）。
- 非抢占：线程获得的资源（例如锁），不能被抢占。
- 循环等待：线程之间存在一个环路，环路上每个线程都额外持有一个资源，而这个资源又是下一个线程要申请的。

这4个条件只要有一个没满足，死锁就不会发生。对于前三个，我们几乎不能寻求改变，因为这是锁的意义，我们能做的是改变循环等待

**预防死锁**

**循环等待** 

也许最实用的预防技术（当然也是经常采用的），就是让代码不会产生循环等待。最直接的方法就是获取锁时提供一个全序（total ordering）。假如系统共有两个锁（L1 和 L2）， 那么我们每次都先申请 L1 然后申请 L2，就可以避免死锁。这样严格的顺序避免了循环等待， 也就不会产生死锁。 

当然，更复杂的系统中不会只有两个锁，锁的全序可能很难做到。因此，偏序（partial ordering）可能是一种有用的方法，安排锁的获取并避免死锁。Linux 中的内存映射代码就是一个偏序锁的好例子。代码开头的注释表明了 10 组不同的加锁顺序，包括简单的关系，比如 i_mutex 早于 i_mmap_mutex，也包括复杂的关系，比如 i_mmap_mutex 早于 private_lock，早于 swap_lock，早于 mapping->tree_lock。 你可以想到，全序和偏序都需要细致的锁策略的设计和实现。另外，顺序只是一种约定，粗心的程序员很容易忽略，导致死锁。最后，有序加锁需要深入理解代码库，了解各种函数的调用关系，即使一个错误，也会导致“D”字。

![image-20230423184153864](/images/mdpic/image-20230423184153864.png)

**持有并等待**

死锁的持有并等待条件，可以通过原子地枪锁来避免。如下：

```c
lock(prevention);
lock(L1);
lock(L2);
...
unlock(prevention); 
```

先抢到 prevention 这个锁之后，代码保证了在抢锁的过程中，不会有不合时宜的线程切 换，从而避免了死锁。当然，这需要任何线程在任何时候抢占锁时，先抢到全局的 prevention 锁。例如，如果另一个线程用不同的顺序抢锁 L1 和 L2，也不会有问题，因为此时，线程已经抢到了 prevention 锁。 

注意，出于某些原因，这个方案也有问题。和之前一样，它不适用于封装：因为这个方案需要我们准确地知道要抢哪些锁，并且提前抢到这些锁。因为要提前抢到所有锁（同 时），而不是在真正需要的时候，所以可能降低了并发。



**非抢占**

在调用 unlock 之前，都认为锁是被占有的，多个抢锁操作通常会带来麻烦，因为我们等待一个锁时，同时持有另一个锁。很多线程库提供更为灵活的接口来避免这种情况。具体来说，trylock()函数会尝试获得锁，或者返回−1，表示锁已经被占有。你可以稍后重试 一下。

可以用这一接口来实现无死锁的加锁方法：

```c
top:
    lock(L1);
    if (trylock(L2) == -1) {		// 如果获取不到L2,则释放L1
    	unlock(L1);
    	goto top;
	} 
```

注意，另一个线程可以使用相同的加锁方式，但是不同的加锁顺序（L2 然后 L1），程序仍然不会产生死锁。但是会引来一个新的问题：***活锁（livelock）***。两个线程有可能一直重复这一序列，又同时都抢锁失败。这种情况下，系统一直在运行这段代码（因此不是死锁），但是又不会有进展，因此名为活锁。也有活锁的解决方法：例如，可以在循环结束的时候，先随机等待一个时间，然后再重复整个动作，这样可以降低线程之间的重复互相干扰。

关于这个方案的最后一点：使用 trylock 方法可能会有一些困难。第一个问题仍然是封装：如果其中的某一个锁，是封装在函数内部的，那么这个跳回开始处就很难实现。如果 代码在中途获取了某些资源，必须要确保也能释放这些资源。例如，在抢到 L1 后，我们的代码分配了一些内存，当抢 L2 失败时，并且在返回开头之前，需要释放这些内存。当然， 在某些场景下（例如，之前提到的 Java 的 vector 方法），这种方法很有效。

**互斥**

最后的预防方法是完全避免互斥。通常来说，代码都会存在临界区，因此很难避免互斥。那么我们应该怎么做呢？ Herlihy 提出了设计各种***无等待（wait-free）*数据结构**的思想。想法很简单：**通过强大的硬件指令，我们可以构造出不需要锁的数据结构。** 举个简单的例子，假设我们有比较并交换（compare-and-swap）指令，是一种由硬件提供的原子指令，做下面的事：

```C
int CompareAndSwap(int *address, int expected, int new){
    if (*address == expected){
        *address = new;
        return 1; // success
    }
    return 0; // failure
}
```

假定我们想原子地给某个值增加特定的数量。我们可以这样实现：

```C
void AtomicIncrement(int *value, int amount){
    do{
        int old = *value;
    } while (CompareAndSwap(value, old, old + amount) == 0);
}
```

无须获取锁，更新值，然后释放锁这些操作，我们使用比较并交换指令，反复尝试将值更新到新的值。这种方式没有使用锁，因此不会有死锁（有可能产生活锁）。 我们来考虑一个更复杂的例子：链表插入。这是在链表头部插入元素的代码：

```C
void insert(int value){
    node_t *n = malloc(sizeof(node_t));
    assert(n != NULL);
    n->value = value;
    n->next = head;
    head = n;
}
```

这段代码在多线程同时调用的时候，会有临界区（看看你是否能弄清楚原因）。当然， 我们可以通过给相关代码加锁，来解决这个问题：

```C
void insert(int value){
    node_t *n = malloc(sizeof(node_t));
    assert(n != NULL);
    n->value = value;
    lock(listlock); // begin critical section
    n->next = head;
    head = n;
    unlock(listlock); // end of critical section
}
```

上面的方案中，我们使用了传统的锁。这里我们尝试用比较并交换指令（compare-and-swap) 来实现插入操作。一种可能的实现是：

```C
void insert(int value) {
    node_t *n = malloc(sizeof(node_t));
    assert(n != NULL);
    n->value = value;
    do {
    	n->next = head;
    } 	while (CompareAndSwap(&head, n->next, n) == 0);
} 
```

这段代码，首先把 next 指针指向当前的链表头（head），然后试着把新节点交换到链表头。但是，如果此时其他的线程成功地修改了 head 的值，这里的交换就会失败，导致这个线程根据新的 head 值重试。 当然，只有插入操作是不够的，要实现一个完善的链表还需要删除、查找等其他工作。 如果你有兴趣，可以去查阅关于无等待同步的丰富文献。(类似自旋锁？)

**通过调度避免死锁**

除了死锁预防，某些场景更适合死锁避免（avoidance）。我们需要了解全局的信息，包括不同线程在运行中对锁的需求情况，从而使得后续的调度能够避免产生死锁。

![image-20230423184433120](/images/mdpic/image-20230423184433120.png)

Dijkstra 提出的银行家算法是一种类似的著名解决方案，文献中也描述了其他类似的方案。遗憾的是，这些方案的适用场景很局限。例如，在嵌入式系统中，你知道所有任务以及它们需要的锁。另外，和上文的第二个例子一样，这种方法会限制并发。因此，通过调度来避免死锁不是广泛使用的通用方案。



**检查和恢复**

最后一种常用的策略就是允许死锁偶尔发生，检查到死锁时再采取行动。举个例子， 如果一个操作系统一年死机一次，你会重启系统，然后愉快地（或者生气地）继续工作。 如果死锁很少见，这种不是办法的办法也是很实用的。

很多数据库系统使用了死锁检测和恢复技术。死锁检测器会定期运行，**通过构建资源图来检查循环**。当循环（死锁）发生时，系统需要重启。如果还需要更复杂的数据结构相 关的修复，那么需要人工参与。 读者可以在其他地方找到更多的关于数据库并发、死锁和相关问题的资料。 阅读这些著作，当然最好可以通过学习数据库的课程，深入地了解这一有趣而且丰富的主题。

####	4.小结

在本章中，我们学习了并发编程中出现的缺陷的类型。第一种是非常常见的，非死锁缺陷，通常也很容易修复。这种问题包括：**违法原子性**，即应该一起执行的指令序列没有 一起执行；**违反顺序**，即两个线程所需的顺序没有强制保证。

同时，我们简要地讨论了死锁：为何会发生，以及如何处理。这个问题几乎和并发一样古老，已经有成百上千的相关论文了。实践中是自行设计抢锁的顺序，从而避免死锁发生。无等待的方案也很有希望，在一些通用库和系统中，包括 Linux，都已经有了一些无等待的实现。然而，这种方案不够通用，并且设计一个新的无等待的数据结构极其复杂，以 至于不够实用。也许，最好的解决方案是开发一种新的并发编程模型：在类似 MapReduce （来自 Google）这样的系统中，程序员可以完成一些类型的并行计算，无须任何锁。 锁必然带来各种困难，也许我们应该尽可能地避免使用锁，除非确信必须使用。



##	33 基于事件的并发（进阶）



##	34 并发的总结对话

- Concurrent是操作系统中最困难的部分
- 尽可能使用锁或生产者-消费者模式写并发程序
- 只要在确实需要的时候才并发，尽可能不用它。过早地优化往往是糟糕地

